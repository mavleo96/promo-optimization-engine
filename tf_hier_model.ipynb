{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install azure-storage-blob\n",
    "# !pip install python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from setup_utils import fetch_data, load_data, create_time_index\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "CONNECTION_STRING = os.getenv(\"CONNECTION_STRING\")\n",
    "\n",
    "load_dotenv()\n",
    "fetch_data(CONNECTION_STRING)\n",
    "\n",
    "(\n",
    "    brand_mapping_backup,\n",
    "    macro_data_backup,\n",
    "    brand_constraint_backup,\n",
    "    pack_constraint_backup,\n",
    "    segment_constraint_backup,\n",
    "    sales_data_backup,\n",
    "    volume_variation_constraint_backup,\n",
    ") = load_data()\n",
    "\n",
    "(\n",
    "    macro_data_backup,\n",
    "    sales_data_backup,\n",
    ") = create_time_index([macro_data_backup, sales_data_backup])\n",
    "\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_mapping = brand_mapping_backup.copy(deep=True)\n",
    "macro_data = macro_data_backup.copy(deep=True)\n",
    "brand_constraint = brand_constraint_backup.copy(deep=True)\n",
    "pack_constraint = pack_constraint_backup.copy(deep=True)\n",
    "segment_constraint = segment_constraint_backup.copy(deep=True)\n",
    "sales_data = sales_data_backup.copy(deep=True)\n",
    "volume_variation_constraint = volume_variation_constraint_backup.copy(deep=True)\n",
    "\n",
    "sales_index = sales_data.index.unique()\n",
    "macro_data = macro_data.loc[sales_index].sort_index()\n",
    "covid = pd.Series([1 if (i<=datetime(2020,5,1) and i>=datetime(2020,3,1)) else 0 for i in macro_data.index], index=sales_index, name=\"covid\")\n",
    "macro_data = macro_data.join(covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_dict = {\n",
    "    \"brand\" : brand_constraint,\n",
    "    \"pack\" : pack_constraint,\n",
    "    \"segment\" : segment_constraint,\n",
    "    \"volume_variation\" : volume_variation_constraint\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = sales_data[sales_data.gto.isna()].reset_index()\n",
    "temp_data[\"month\"] = temp_data.date.dt.month\n",
    "temp_data[\"year\"] = temp_data.date.dt.year\n",
    "temp_data = temp_data.fillna(10000)\n",
    "temp_data = temp_data.merge(brand_mapping)\n",
    "\n",
    "master_mapping = temp_data[[\"sku\", \"pack\", \"brand\", \"segment\"]].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_encodings(master_map):\n",
    "\n",
    "    def label_encoder(series):\n",
    "        unique_values = series.sort_values().unique()\n",
    "        unique_count =  series.nunique()\n",
    "\n",
    "        return dict(zip(unique_values, range(len(unique_values))))\n",
    "    \n",
    "    def mapper(col_val, col_key=\"sku\"):\n",
    "\n",
    "        df = master_map[[col_key, col_val]].drop_duplicates()\n",
    "        df.loc[:,col_val] = df[col_val].map(label_dict[col_val])\n",
    "        df.loc[:,col_key] = df[col_key].map(label_dict[col_key])\n",
    "\n",
    "        return df.set_index(col_key).to_dict()[col_val]\n",
    "\n",
    "    label_dict = {col:label_encoder(master_map[col]) for col in master_map.columns}\n",
    "    mapper_dict = {col:mapper(col) for col in master_map.columns if col!=\"sku\"}\n",
    "\n",
    "    return {\"label_dict\" : label_dict, \"mapper_dict\" : mapper_dict}\n",
    "\n",
    "final_encodings = _create_encodings(master_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sku_list = np.sort(sales_data.sku.unique()).tolist()\n",
    "target_sku_list = list(final_encodings[\"label_dict\"][\"sku\"].keys())\n",
    "non_target_sku_list = [i for i  in total_sku_list if i not in final_encodings[\"label_dict\"][\"sku\"]]\n",
    "sku_index_order = [*target_sku_list, *non_target_sku_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _constraint_tensor_generate(constraint, encoding, key):\n",
    "\n",
    "    encoding_length = max(encoding[\"label_dict\"][key].values())+1\n",
    "    constraint = constraint.copy(deep=True)\n",
    "    constraint = constraint.replace(encoding[\"label_dict\"][key]).sort_values([\"month\", key])\n",
    "    constraint = constraint.groupby([\"month\", key]).max_discount.sum().sort_index().unstack(1)\n",
    "\n",
    "    constraint = pd.DataFrame(columns=pd.Index(range(0,encoding_length), dtype='int64', name=\"brand\"), index=pd.Index(range(6,8), dtype='int64')).fillna(constraint).fillna(0.0).to_numpy()\n",
    "\n",
    "    return constraint\n",
    "\n",
    "brand_constraint_tensor = _constraint_tensor_generate(constraints_dict[\"brand\"], final_encodings, \"brand\")\n",
    "pack_constraint_tensor = _constraint_tensor_generate(constraints_dict[\"pack\"], final_encodings, \"pack\")\n",
    "segment_constraint_tensor = _constraint_tensor_generate(constraints_dict[\"segment\"], final_encodings, \"segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_data = macro_data.loc[sales_index].sort_index()\n",
    "macro_data = (macro_data/macro_data.mean()-1).copy(deep=True)\n",
    "macro_data = macro_data.astype(np.float64).values\n",
    "macro_data = np.expand_dims(macro_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_data = (\n",
    "    sales_data\n",
    "    .reset_index()\n",
    "    .groupby([\"date\", \"sku\"])\n",
    "    .net_revenue.sum()\n",
    "    .sort_index()\n",
    "    .unstack(1)\n",
    "    [sku_index_order]\n",
    "    .clip(0.0, None)\n",
    "    .fillna(0.0)\n",
    "    .astype(np.float64)\n",
    "    .values\n",
    ")\n",
    "nr_data_mask = (\n",
    "    sales_data\n",
    "    .reset_index()\n",
    "    .groupby([\"date\", \"sku\"])\n",
    "    .net_revenue.sum()\n",
    "    .sort_index()\n",
    "    .unstack(1)\n",
    "    [sku_index_order]\n",
    "    .applymap(lambda x: x if x>=0 else np.nan)\n",
    "    .notna()\n",
    "    .astype(np.float64)\n",
    "    .values\n",
    ")\n",
    "\n",
    "nr_shifted = (\n",
    "    sales_data\n",
    "    .reset_index()\n",
    "    .groupby([\"date\", \"sku\"])\n",
    "    .net_revenue.sum()\n",
    "    .sort_index()\n",
    "    .unstack(1)\n",
    "    [sku_index_order]\n",
    "    .applymap(lambda x: x if x>=0 else np.nan)\n",
    "    .clip(0.0, None)\n",
    "    .shift(1)\n",
    "    .fillna(method=\"bfill\")\n",
    "    .fillna(0.0)\n",
    "    .astype(np.float64)\n",
    "    .values\n",
    ")\n",
    "\n",
    "volume_data = (\n",
    "    sales_data\n",
    "    .reset_index()\n",
    "    .groupby([\"date\", \"sku\"])\n",
    "    .volume.sum()\n",
    "    .sort_index()\n",
    "    .unstack(1)\n",
    "    [sku_index_order]\n",
    "    .clip(0.0, None)\n",
    "    .fillna(0.0)\n",
    "    .astype(np.float64)\n",
    "    .values\n",
    ")\n",
    "\n",
    "\n",
    "discount_data = (\n",
    "    sales_data\n",
    "    .reset_index()\n",
    "    .groupby([\"date\", \"sku\"])[[\"promotional_discount\", \"other_discounts\"]].sum()\n",
    "    .sort_index()\n",
    "    .stack()\n",
    "    .unstack(1)\n",
    "    [sku_index_order]\n",
    "    .fillna(0.0)\n",
    "    .clip(None, 0)\n",
    ")\n",
    "discount_data = np.swapaxes(discount_data.astype(np.float64).values.reshape(55,2,discount_data.shape[1]), 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = nr_data.mean()\n",
    "vol_scaler = volume_data.mean()\n",
    "\n",
    "nr_data = nr_data/scaler\n",
    "discount_data = discount_data/scaler\n",
    "nr_shifted = nr_shifted/scaler\n",
    "brand_constraint_tensor = brand_constraint_tensor/scaler\n",
    "pack_constraint_tensor = pack_constraint_tensor/scaler\n",
    "segment_constraint_tensor = segment_constraint_tensor/scaler\n",
    "\n",
    "\n",
    "volume_data = volume_data/vol_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index_array = np.expand_dims(np.arange(1, macro_data.shape[0]+1), 1)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "y = tf.constant(nr_data, dtype=tf.float64)\n",
    "y_mask = tf.constant(nr_data_mask, dtype=tf.float64)\n",
    "\n",
    "discounts = tf.constant(discount_data, dtype=tf.float64)\n",
    "mixed_effect = tf.constant(macro_data, dtype=tf.float64)\n",
    "time_index = tf.constant(np.expand_dims(np.arange(1, macro_data.shape[0]+1), 1), dtype=tf.float64)\n",
    "shifted_nr = tf.constant(nr_shifted, dtype=tf.float64)\n",
    "y_vol = tf.constant(volume_data, dtype=tf.float64)\n",
    "\n",
    "val_splitter_ = tf.constant(5, dtype=tf.int32)\n",
    "val_splitter = 5 #if val_splitter_ == 5 else 2\n",
    "\n",
    "initial_discount_var = tf.slice(discounts, begin=[0,0,0], size=[2,-1,2])\n",
    "\n",
    "mixed_effect_var = tf.slice(mixed_effect, begin=[0,0,0], size=[2,-1,-1])\n",
    "time_index_var = tf.slice(time_index, begin=[0,0], size=[2,-1])\n",
    "y_mask_var = tf.slice(y_mask, begin=[0,0], size=[2,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "#Y\n",
    "y = tf.compat.v1.placeholder(dtype=tf.float64, name=\"nr_actual\")\n",
    "y_mask = tf.compat.v1.placeholder(dtype=tf.float64, name=\"nr_mask\")\n",
    "\n",
    "# X\n",
    "discounts = tf.compat.v1.placeholder(dtype=tf.float64, name=\"discounts\")\n",
    "mixed_effect = tf.compat.v1.placeholder(dtype=tf.float64, name=\"mixed_effects\")\n",
    "time_index = tf.compat.v1.placeholder(dtype=tf.float64, name=\"time_index\")\n",
    "# shifted_nr = tf.compat.v1.placeholder(dtype=tf.float64, name=\"shifted_nr\")\n",
    "y_vol = tf.compat.v1.placeholder(dtype=tf.float64, name=\"volume_actual\")\n",
    "\n",
    "val_splitter_ = tf.compat.v1.placeholder(dtype=tf.int32)\n",
    "val_splitter = 5 #if val_splitter_ == 5 else 2\n",
    "\n",
    "initial_discount_var = tf.compat.v1.placeholder(dtype=tf.float64, name=\"initial_discount_submit\")\n",
    "\n",
    "mixed_effect_var = tf.compat.v1.placeholder(dtype=tf.float64, name=\"mixed_effect_submit\")\n",
    "time_index_var = tf.compat.v1.placeholder(dtype=tf.float64, name=\"time_index_submit\")\n",
    "y_mask_var = tf.compat.v1.placeholder(dtype=tf.float64, name=\"y_mask_submit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_size = (1,nr_data.shape[1])\n",
    "me_size = macro_data.shape[-1]\n",
    "\n",
    "baseline_intercept = tf.Variable(np.expand_dims((nr_data.mean(0)*0.3), 0), dtype=tf.float64)\n",
    "\n",
    "baseline_slope1_global = tf.Variable(np.full((1,1), 0.1), dtype=tf.float64)\n",
    "baseline_slope1_hier = tf.Variable(np.full(dim_size, 0.1), dtype=tf.float64)\n",
    "\n",
    "mixed_effect_mult_global = tf.Variable(np.random.normal(loc=0, size=(1, 1, me_size)), dtype=tf.float64)\n",
    "mixed_effect_mult_hier = tf.Variable(np.random.normal(loc=0, size=(*dim_size, me_size)), dtype=tf.float64)\n",
    "\n",
    "discount_slope_global = tf.math.sigmoid(tf.Variable(np.random.normal(loc=0, size=(1, 1, 2)), dtype=tf.float64))*-3\n",
    "discount_slope_hier = tf.math.sigmoid(tf.Variable(np.random.normal(loc=0, size=(*dim_size, 2)), dtype=tf.float64))*-3\n",
    "\n",
    "roi_mults_global = tf.Variable(np.random.normal(loc=0, size=(1, 1, me_size)), dtype=tf.float64)\n",
    "roi_mults_hier = tf.Variable(np.random.normal(loc=0, size=(*dim_size, me_size)), dtype=tf.float64)\n",
    "\n",
    "nr_to_vol_slope = tf.Variable(np.random.normal(loc=0, size=dim_size), dtype=tf.float64)\n",
    "\n",
    "\n",
    "hier_var_list = [baseline_slope1_hier, mixed_effect_mult_hier, discount_slope_hier, roi_mults_hier] #baseline_slope2_hier\n",
    "global_var_list = [baseline_slope1_global, mixed_effect_mult_global, discount_slope_global, roi_mults_global, nr_to_vol_slope] #baseline_slope2_global\n",
    "\n",
    "discounts_var = tf.Variable(initial_discount_var, dtype=tf.float64)\n",
    "sliced_discount_var = tf.slice(discounts_var, begin=[0,0,0], size=[2,151,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def model(\n",
    "        base_intercept_in,\n",
    "        base_slope1_global_in,\n",
    "        base_slope1_hier_in,\n",
    "        # base_slope2_global_in,\n",
    "        # base_slope2_hier_in,\n",
    "        mixed_effect_mult_global_in,\n",
    "        mixed_effect_mult_hier_in,\n",
    "        discount_slope_global_in,\n",
    "        discount_slope_hier_in,\n",
    "        roi_mults_global_in,\n",
    "        roi_mults_hier_in,\n",
    "        nr_to_vol_slope_in,\n",
    "        time_index_in,\n",
    "        mixed_effect_in,\n",
    "        discounts_in,\n",
    "        y_mask_in,\n",
    "    ):\n",
    "    base_slope1_in = base_slope1_global_in + base_slope1_hier_in\n",
    "    # base_slope2_in = base_slope2_global_in + base_slope2_hier_in\n",
    "    mixed_effect_mult_in = mixed_effect_mult_global_in + mixed_effect_mult_hier_in\n",
    "    discount_slope_in = discount_slope_global_in + discount_slope_hier_in\n",
    "    roi_mults_in = roi_mults_global_in + roi_mults_hier_in\n",
    "\n",
    "    base1_in = tf.multiply(base_slope1_in, time_index_in) + base_intercept_in\n",
    "    base2_in = base1_in #+ tf.multiply(base_slope2_in, shifted_nr)\n",
    "    mixed_effect_impact_in = 1 + tf.nn.tanh(tf.multiply(mixed_effect_in, mixed_effect_mult_in))\n",
    "    total_mixed_effect_impact_in = tf.reduce_prod(mixed_effect_impact_in, axis=-1)\n",
    "    discount_impact_in = tf.multiply(discount_slope_in, discounts_in)\n",
    "    roi_mult_impact_in = 1 + tf.nn.tanh(tf.multiply(mixed_effect_impact_in, roi_mults_in))\n",
    "    total_roi_mult_impact_in = tf.expand_dims(tf.reduce_prod(roi_mult_impact_in, axis=1), axis=1)\n",
    "\n",
    "    y_pred_out = tf.multiply(\n",
    "        y_mask_in,\n",
    "        (\n",
    "            tf.multiply(base2_in, total_mixed_effect_impact_in)\n",
    "            + tf.reduce_sum(discount_impact_in, axis=-1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    y_vol_pred_out = tf.multiply(y_pred_out, nr_to_vol_slope_in)\n",
    "\n",
    "    return y_pred_out, y_vol_pred_out\n",
    "\n",
    "@tf.function\n",
    "def wape(y_actual, y_prediction):\n",
    "    return tf.reduce_sum(tf.math.abs(y_actual - y_prediction))/tf.reduce_sum(y_actual)\n",
    "\n",
    "@tf.function\n",
    "def mse(y_actual, y_prediction):\n",
    "    return tf.reduce_sum(tf.math.square(y_actual - y_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_vol_pred = model(\n",
    "    baseline_intercept,\n",
    "    baseline_slope1_global,\n",
    "    baseline_slope1_hier,\n",
    "    # baseline_slope2_global,\n",
    "    # baseline_slope2_hier_in,\n",
    "    mixed_effect_mult_global,\n",
    "    mixed_effect_mult_hier,\n",
    "    discount_slope_global,\n",
    "    discount_slope_hier,\n",
    "    roi_mults_global,\n",
    "    roi_mults_hier,\n",
    "    nr_to_vol_slope,\n",
    "    time_index,\n",
    "    mixed_effect,\n",
    "    discounts,\n",
    "    y_mask,\n",
    ")\n",
    "\n",
    "y_split = tf.split(y, val_splitter)\n",
    "y_pred_split = tf.split(y_pred, val_splitter)\n",
    "\n",
    "y_vol_split = tf.split(y_vol, val_splitter)\n",
    "y_vol_pred_split = tf.split(y_vol_pred, val_splitter)\n",
    "\n",
    "\n",
    "# loss\n",
    "total_wape = tf.math.reduce_mean([wape(y_split[i], y_pred_split[i]) for i in range(0,val_splitter)])\n",
    "total_mse = mse(y, y_pred)\n",
    "actual_wape = wape(y, y_pred)\n",
    "\n",
    "total_wape_vol = tf.math.reduce_mean([wape(y_vol_split[i], y_vol_pred_split[i]) for i in range(0,val_splitter)])\n",
    "total_mse_vol = mse(y_vol, y_vol_pred)\n",
    "actual_wape_vol = wape(y_vol, y_vol_pred)\n",
    "\n",
    "\n",
    "reg1 = sum([tf.reduce_sum(tf.square(i)) for i in hier_var_list])\n",
    "reg2 = sum([tf.reduce_sum(tf.square(i)) for i in global_var_list])\n",
    "\n",
    "loss = (\n",
    "    1e3*total_wape_vol\n",
    "    +1e1*total_mse_vol\n",
    "    +1e3*total_wape\n",
    "    +1e1*total_mse\n",
    "    +1e3*reg2\n",
    "    +1e1*reg1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _tensor_gather(tensor_to_gather_in, encoding, key):\n",
    "    encoding = pd.Series(encoding[\"mapper_dict\"][key]).sort_index().to_numpy()\n",
    "    segment_ids = tf.constant(encoding, dtype=tf.int32)\n",
    "    x_transpose = tf.transpose(tensor_to_gather_in, perm=[1,0,2])\n",
    "    x_gathered = tf.math.unsorted_segment_sum(x_transpose, segment_ids, num_segments=encoding.max()+1)\n",
    "    x_gathered_transpose = tf.reduce_mean(tf.transpose(x_gathered, perm=[1,0,2]), axis=2)\n",
    "\n",
    "    return x_gathered_transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_inital_pred, y_initial_vol_pred = model(\n",
    "    baseline_intercept,\n",
    "    baseline_slope1_global,\n",
    "    baseline_slope1_hier,\n",
    "    # baseline_slope2_global,\n",
    "    # baseline_slope2_hier_in,\n",
    "    mixed_effect_mult_global,\n",
    "    mixed_effect_mult_hier,\n",
    "    discount_slope_global,\n",
    "    discount_slope_hier,\n",
    "    roi_mults_global,\n",
    "    roi_mults_hier,\n",
    "    nr_to_vol_slope,\n",
    "    time_index_var,\n",
    "    mixed_effect_var,\n",
    "    initial_discount_var,\n",
    "    y_mask_var,\n",
    ")\n",
    "\n",
    "slice_y_inital_pred = tf.slice(y_inital_pred, begin=[0,0], size=[-1, 151])\n",
    "slice_y_initial_vol_pred = tf.slice(y_initial_vol_pred, begin=[0,0], size=[-1, 151])\n",
    "\n",
    "\n",
    "y_opt_pred, y_opt_vol_pred = model(\n",
    "    baseline_intercept,\n",
    "    baseline_slope1_global,\n",
    "    baseline_slope1_hier,\n",
    "    # baseline_slope2_global,\n",
    "    # baseline_slope2_hier_in,\n",
    "    mixed_effect_mult_global,\n",
    "    mixed_effect_mult_hier,\n",
    "    discount_slope_global,\n",
    "    discount_slope_hier,\n",
    "    roi_mults_global,\n",
    "    roi_mults_hier,\n",
    "    nr_to_vol_slope,\n",
    "    time_index_var,\n",
    "    mixed_effect_var,\n",
    "    discounts_var,\n",
    "    y_mask_var,\n",
    ")\n",
    "\n",
    "slice_y_opt_pred = tf.slice(y_opt_pred, begin=[0,0], size=[-1, 151])\n",
    "slice_y_opt_vol_pred = tf.slice(y_opt_vol_pred, begin=[0,0], size=[-1, 151])\n",
    "\n",
    "\n",
    "discount_var_brand = _tensor_gather(sliced_discount_var, final_encodings, \"brand\")\n",
    "discount_var_pack = _tensor_gather(sliced_discount_var, final_encodings, \"pack\")\n",
    "discount_var_segment = _tensor_gather(sliced_discount_var, final_encodings, \"segment\")\n",
    "\n",
    "brand_constraint_loss = tf.reduce_sum(tf.nn.relu(brand_constraint_tensor - discount_var_brand))\n",
    "pack_constraint_loss = tf.reduce_sum(tf.nn.relu(pack_constraint_tensor - discount_var_pack))\n",
    "segment_constraint_loss = tf.reduce_sum(tf.nn.relu(segment_constraint_tensor - discount_var_segment))\n",
    "\n",
    "roi = tf.divide(tf.reduce_sum(slice_y_opt_pred - slice_y_inital_pred), -tf.reduce_sum(discounts_var))\n",
    "\n",
    "loss_roi = -1e2*roi + 1e1*brand_constraint_loss + 1e1*pack_constraint_loss + 1e1*segment_constraint_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = 40\n",
    "\n",
    "feed_dict1 = {\n",
    "    discounts : discount_data[:splitter],\n",
    "    mixed_effect: macro_data[:splitter],\n",
    "    y_vol : volume_data[:splitter],\n",
    "    y : nr_data[:splitter],\n",
    "    # shifted_nr : nr_shifted[:splitter],\n",
    "    y_mask : nr_data_mask[:splitter],\n",
    "    time_index : time_index_array[:splitter],\n",
    "    val_splitter_ : 5,\n",
    "    initial_discount_var : discount_data[-2:],\n",
    "    mixed_effect_var : macro_data[-2:],\n",
    "    time_index_var : time_index_array[-2:],\n",
    "    y_mask_var : nr_data_mask[-2:]\n",
    "}\n",
    "\n",
    "feed_dict2 = {\n",
    "    discounts : discount_data[splitter:-5],\n",
    "    mixed_effect: macro_data[splitter:-5],\n",
    "    y_vol : volume_data[splitter:-5],\n",
    "    y : nr_data[splitter:-5],\n",
    "    # shifted_nr : nr_shifted[splitter:-5],\n",
    "    y_mask : nr_data_mask[splitter:-5],\n",
    "    time_index : time_index_array[splitter:-5],\n",
    "    val_splitter_ : 5,\n",
    "    initial_discount_var : discount_data[-2:],\n",
    "    mixed_effect_var : macro_data[-2:],\n",
    "    time_index_var : time_index_array[-2:],\n",
    "    y_mask_var : nr_data_mask[-2:]\n",
    "}\n",
    "\n",
    "# feed_dict3 = {\n",
    "#     discounts : discount_data[-2:],\n",
    "#     mixed_effect: macro_data[-2:],\n",
    "#     y_vol : volume_data[-2:],\n",
    "#     y : nr_data[-2:],\n",
    "#     # shifted_nr : nr_shifted[-2:],\n",
    "#     y_mask : nr_data_mask[-2:],\n",
    "#     time_index : time_index_array[-2:],\n",
    "#     val_splitter_ : 5\n",
    "# }\n",
    "\n",
    "\n",
    "initial_discount_var = tf.compat.v1.placeholder(dtype=tf.float64, name=\"initial_discount_submit\")\n",
    "\n",
    "mixed_effect_var = tf.compat.v1.placeholder(dtype=tf.float64, name=\"mixed_effect_submit\")\n",
    "time_index_var = tf.compat.v1.placeholder(dtype=tf.float64, name=\"time_index_submit\")\n",
    "y_mask_var = tf.compat.v1.placeholder(dtype=tf.float64, name=\"y_mask_submit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "# optimizer\n",
    "lr = lambda x : 1 / np.power(x/5 + 10, 1/2)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=lr(epoch))#, beta1=0.1, beta2=0.1)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lr(i) for i in [0, 1, 10, 100, 1000, 10000, 20000]]#, 50000, 80000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "# optimizer\n",
    "lr2 = lambda x : 1 / np.power(x/5 + 10, 1/2)\n",
    "optimizer_roi = tf.compat.v1.train.AdamOptimizer(learning_rate=lr2(epoch))#, beta1=0.1, beta2=0.1)\n",
    "train_roi = optimizer_roi.minimize(loss_roi, var_list=[discounts_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize variables\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(init, feed_dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_update_track = {\n",
    "    \"epoch\" : [],\n",
    "    \"actual_wape\" : [],\n",
    "    \"test_wape\" : [],\n",
    "    \"loss\" : [],\n",
    "    \"mse\" : [],\n",
    "    \"reg1\" : [],\n",
    "    \"reg2\" : []\n",
    "}\n",
    "\n",
    "# train model\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    (\n",
    "        _,\n",
    "        current_loss,\n",
    "        current_wape,\n",
    "        # current_mse,\n",
    "        current_wape_vol,\n",
    "        # current_mse_vol,\n",
    "        current_reg1,\n",
    "        current_reg2\n",
    "    )= sess.run([\n",
    "        train,\n",
    "        loss,\n",
    "        actual_wape,\n",
    "        # total_mse,\n",
    "        actual_wape_vol,\n",
    "        # total_mse_vol,\n",
    "        reg1,\n",
    "        reg2\n",
    "    ], feed_dict1)\n",
    "\n",
    "    current_wape_test, current_wape_vol_test = sess.run([actual_wape, actual_wape_vol], feed_dict2)\n",
    "\n",
    "\n",
    "    if (epoch + 1) % 250 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {current_loss:.4f}, WAPE: {current_wape:.4f}, WAPE_TEST: {current_wape_test:.4f}, WAPE_VOL: {current_wape_vol:.4f}, WAPE_VOL_TEST: {current_wape_vol_test:.4f}, reg1: {current_reg1:.4f}, reg2: {current_reg2:.4f}\")\n",
    "        # metric_update_track[\"epoch\"].append(epoch)\n",
    "        # metric_update_track[\"actual_wape\"].append(current_wape)\n",
    "        # metric_update_track[\"test_wape\"].append(current_wape_test)\n",
    "        # metric_update_track[\"loss\"].append(current_loss)\n",
    "        # metric_update_track[\"mse\"].append(current_mse)\n",
    "        # metric_update_track[\"reg1\"].append(current_reg1)\n",
    "        # metric_update_track[\"reg2\"].append(current_reg2)\n",
    "\n",
    "\n",
    "\n",
    "#         # Training loop\n",
    "# num_epochs = 500\n",
    "# for epoch in range(num_epochs):\n",
    "#     _, current_error, cuurent_mse, current_m1, current_m2, current_c = sess.run([train_op, error, mse_error, m1, m2, c])\n",
    "#     if (epoch + 1) % 25 == 0:\n",
    "#         print(f\"Epoch {epoch + 1}/{num_epochs}, Error: {current_error:.4f}, MSE: {cuurent_mse:.4f}, m1: {current_m1}, m2: {current_m2}, c: {current_c}\")\n",
    "\n",
    "# # Print the final results for 'm' and 'c'\n",
    "# final_m1, final_m2, final_c = sess.run([m1, m2, c])\n",
    "# print(f\"Final 'm1' value: {final_m1}\")\n",
    "\n",
    "# print(f\"Final 'm2' value: {final_m2}\")\n",
    "# print(f\"Final 'c' value: {final_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_update_track = {\n",
    "    \"epoch\" : [],\n",
    "    \"actual_wape\" : [],\n",
    "    \"test_wape\" : [],\n",
    "    \"loss\" : [],\n",
    "    \"mse\" : [],\n",
    "    \"reg1\" : [],\n",
    "    \"reg2\" : []\n",
    "}\n",
    "\n",
    "# train model\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    (\n",
    "        _,\n",
    "        current_loss_roi,\n",
    "        current_roi,\n",
    "        current_brand_constraint_loss,\n",
    "        current_pack_constraint_loss,\n",
    "        current_segment_constraint_loss,\n",
    "        # current_wape,\n",
    "        # current_mse,\n",
    "        # current_wape_vol,\n",
    "        # current_mse_vol,\n",
    "        # current_reg1,\n",
    "        # current_reg2\n",
    "    )= sess.run([\n",
    "        train_roi,\n",
    "        loss_roi,\n",
    "        roi,\n",
    "        brand_constraint_loss,\n",
    "        pack_constraint_loss,\n",
    "        segment_constraint_loss,\n",
    "        # actual_wape,\n",
    "        # total_mse,\n",
    "        # actual_wape_vol,\n",
    "        # total_mse_vol,\n",
    "        # reg1,\n",
    "        # reg2\n",
    "    ], feed_dict1)\n",
    "\n",
    "    if (epoch + 1) % 250 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {current_loss_roi:.4f}, ROI: {current_roi:.4f}, BRAND_LOSS: {current_brand_constraint_loss:.4f}, PACK_LOSS: {current_pack_constraint_loss:.4f}, SEGMENT_LOSS: {current_segment_constraint_loss:.4f}\")\n",
    "        # \")#, WAPE_TEST: {current_wape_test:.4f}, WAPE_VOL: {current_wape_vol:.4f}, WAPE_VOL_TEST: {current_wape_vol_test:.4f}, reg1: {current_reg1:.4f}, reg2: {current_reg2:.4f}\")\n",
    "        # metric_update_track[\"epoch\"].append(epoch)\n",
    "        # metric_update_track[\"actual_wape\"].append(current_wape)\n",
    "        # metric_update_track[\"test_wape\"].append(current_wape_test)\n",
    "        # metric_update_track[\"loss\"].append(current_loss)\n",
    "        # metric_update_track[\"mse\"].append(current_mse)\n",
    "        # metric_update_track[\"reg1\"].append(current_reg1)\n",
    "        # metric_update_track[\"reg2\"].append(current_reg2)\n",
    "\n",
    "\n",
    "\n",
    "#         # Training loop\n",
    "# num_epochs = 500\n",
    "# for epoch in range(num_epochs):\n",
    "#     _, current_error, cuurent_mse, current_m1, current_m2, current_c = sess.run([train_op, error, mse_error, m1, m2, c])\n",
    "#     if (epoch + 1) % 25 == 0:\n",
    "#         print(f\"Epoch {epoch + 1}/{num_epochs}, Error: {current_error:.4f}, MSE: {cuurent_mse:.4f}, m1: {current_m1}, m2: {current_m2}, c: {current_c}\")\n",
    "\n",
    "# # Print the final results for 'm' and 'c'\n",
    "# final_m1, final_m2, final_c = sess.run([m1, m2, c])\n",
    "# print(f\"Final 'm1' value: {final_m1}\")\n",
    "\n",
    "# print(f\"Final 'm2' value: {final_m2}\")\n",
    "# print(f\"Final 'c' value: {final_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_submit, vol_submit = sess.run([y_pred, y_vol_pred], feed_dict3)\n",
    "nr_submit = nr_submit * scaler\n",
    "vol_submit = vol_submit * vol_scaler\n",
    "\n",
    "nr_data_temp = (\n",
    "    sales_data\n",
    "    .reset_index()\n",
    "    .groupby([\"date\", \"sku\"])\n",
    "    .net_revenue.sum()\n",
    "    .sort_index()\n",
    "    .unstack(1)\n",
    ")\n",
    "nr_submit = pd.DataFrame(nr_submit, index=nr_data_temp.index[-2:], columns=nr_data_temp.columns)\n",
    "vol_submit = pd.DataFrame(vol_submit, index=nr_data_temp.index[-2:], columns=nr_data_temp.columns)\n",
    "\n",
    "submit_temp = sales_data[sales_data.gto.isna()].reset_index().set_index([\"date\", \"sku\", \"brand\", \"pack\", \"size\"]).sort_index()\n",
    "submit_temp.loc[:, \"net_revenue\"] = submit_temp.net_revenue.fillna(nr_submit.stack()).apply(lambda x: x if x>0 else -x/2)\n",
    "submit_temp.loc[:, \"volume\"] = submit_temp.volume.fillna(vol_submit.stack()).apply(lambda x: x if x>0 else -x/2)\n",
    "submit_temp = submit_temp.reset_index()\n",
    "\n",
    "cols_req = [ \"Year\", \"Month\", \"SKU\", \"Brand\", \"Pack\", \"Size\", \"Volume_Estimate\", \"Net_Revenue_Estimate\", \"Optimal_Promotional_Discount\", \"Optimal_Other_Discounts\", \"Optimal_Volume\", \"Optimal_Net_Revenue\"]\n",
    "\n",
    "\n",
    "submit_temp.loc[:, \"Year\"] = submit_temp.date.dt.year\n",
    "submit_temp.loc[:, \"Month\"] = submit_temp.date.dt.month\n",
    "submit_temp.loc[:, \"SKU\"] = submit_temp.sku\n",
    "submit_temp.loc[:, \"Brand\"] = submit_temp.brand\n",
    "submit_temp.loc[:, \"Pack\"] = submit_temp.pack\n",
    "submit_temp.loc[:, \"Size\"] = submit_temp.size\n",
    "submit_temp.loc[:, \"Volume_Estimate\"] = submit_temp.volume\n",
    "submit_temp.loc[:, \"Net_Revenue_Estimate\"] = submit_temp.net_revenue\n",
    "submit_temp.loc[:, \"Optimal_Promotional_Discount\"] = submit_temp.promotional_discount\n",
    "submit_temp.loc[:, \"Optimal_Other_Discounts\"] = submit_temp.other_discounts\n",
    "submit_temp.loc[:, \"Optimal_Volume\"] = submit_temp.volume\n",
    "submit_temp.loc[:, \"Optimal_Net_Revenue\"] = submit_temp.net_revenue\n",
    "\n",
    "submit_temp = submit_temp[cols_req]\n",
    "submit_temp.to_csv(\"/home/akshay-development-server/promo-optimization_team-simpsons-paradox/data/team_simpsons_paradox_submission_1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mroi_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
