{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/brand_segment_mapping_hackathon.xlsx\n",
      "data/macro_data.xlsx\n",
      "data/maximum_discount_constraint_hackathon.xlsx\n",
      "data/sales_data_hackathon.xlsx\n",
      "data/volume_variation_constraint_hackathon.xlsx\n",
      "data/submission_template_hackathon.csv\n"
     ]
    }
   ],
   "source": [
    "# !pip install azure-storage-blob\n",
    "# !pip install python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from setup_utils import fetch_data, load_data, create_time_index\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "CONNECTION_STRING = os.getenv(\"CONNECTION_STRING\")\n",
    "\n",
    "load_dotenv()\n",
    "fetch_data(CONNECTION_STRING)\n",
    "\n",
    "(\n",
    "    brand_mapping_backup,\n",
    "    macro_data_backup,\n",
    "    brand_constraint_backup,\n",
    "    pack_constraint_backup,\n",
    "    segment_constraint_backup,\n",
    "    sales_data_backup,\n",
    "    volume_variation_constraint_backup,\n",
    ") = load_data()\n",
    "\n",
    "(\n",
    "    macro_data_backup,\n",
    "    sales_data_backup,\n",
    ") = create_time_index([macro_data_backup, sales_data_backup])\n",
    "\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_mapping = brand_mapping_backup.copy(deep=True)\n",
    "macro_data = macro_data_backup.copy(deep=True)\n",
    "brand_constraint = brand_constraint_backup.copy(deep=True)\n",
    "pack_constraint = pack_constraint_backup.copy(deep=True)\n",
    "segment_constraint = segment_constraint_backup.copy(deep=True)\n",
    "sales_data = sales_data_backup.copy(deep=True)\n",
    "volume_variation_constraint = volume_variation_constraint_backup.copy(deep=True)\n",
    "\n",
    "sales_index = sales_data.index.unique()\n",
    "macro_data = macro_data.loc[sales_index].sort_index()\n",
    "covid = pd.Series([1 if (i<=datetime(2020,5,1) and i>=datetime(2020,3,1)) else 0 for i in macro_data.index], index=sales_index, name=\"covid\")\n",
    "macro_data = macro_data.join(covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_data = macro_data.loc[sales_index].sort_index()\n",
    "macro_data = (macro_data/macro_data.mean()-1).copy(deep=True)\n",
    "macro_data = macro_data.astype(np.float64).values\n",
    "macro_data = np.expand_dims(macro_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_data = (\n",
    "    sales_data\n",
    "    .reset_index()\n",
    "    .groupby([\"date\", \"sku\"])\n",
    "    .net_revenue.sum()\n",
    "    .sort_index()\n",
    "    .unstack(1)\n",
    "    .clip(0.0, None)\n",
    "    .fillna(0.0)\n",
    "    .astype(np.float64)\n",
    "    .values\n",
    ")\n",
    "nr_data_mask = (\n",
    "    sales_data\n",
    "    .reset_index()\n",
    "    .groupby([\"date\", \"sku\"])\n",
    "    .net_revenue.sum()\n",
    "    .sort_index()\n",
    "    .unstack(1)\n",
    "    .applymap(lambda x: x if x>=0 else np.nan)\n",
    "    .notna()\n",
    "    .astype(np.float64)\n",
    "    .values\n",
    ")\n",
    "\n",
    "nr_shifted = (\n",
    "    sales_data\n",
    "    .reset_index()\n",
    "    .groupby([\"date\", \"sku\"])\n",
    "    .net_revenue.sum()\n",
    "    .sort_index()\n",
    "    .unstack(1)\n",
    "    .applymap(lambda x: x if x>=0 else np.nan)\n",
    "    .clip(0.0, None)\n",
    "    .shift(1)\n",
    "    .fillna(method=\"bfill\")\n",
    "    .fillna(0.0)\n",
    "    .astype(np.float64)\n",
    "    .values\n",
    ")\n",
    "\n",
    "volume_data = (\n",
    "    sales_data\n",
    "    .reset_index()\n",
    "    .groupby([\"date\", \"sku\"])\n",
    "    .volume.sum()\n",
    "    .sort_index()\n",
    "    .unstack(1)\n",
    "    .clip(0.0, None)\n",
    "    .fillna(0.0)\n",
    "    .astype(np.float64)\n",
    "    .values\n",
    ")\n",
    "\n",
    "\n",
    "discount_data = (\n",
    "    sales_data\n",
    "    .reset_index()\n",
    "    .groupby([\"date\", \"sku\"])[[\"promotional_discount\", \"other_discounts\"]].sum()\n",
    "    .sort_index()\n",
    "    .stack()\n",
    "    .unstack(1)\n",
    "    .fillna(0.0)\n",
    "    .clip(None, 0)\n",
    ")\n",
    "discount_data = -np.swapaxes(discount_data.astype(np.float64).values.reshape(55,2,discount_data.shape[1]), 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = nr_data.mean()\n",
    "vol_scaler = volume_data.mean()\n",
    "\n",
    "nr_data = nr_data/scaler\n",
    "discount_data = discount_data/scaler\n",
    "nr_shifted = nr_shifted/scaler\n",
    "\n",
    "volume_data = volume_data/vol_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index_array = np.expand_dims(np.arange(1, macro_data.shape[0]+1), 1)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "y = tf.constant(nr_data, dtype=tf.float64)\n",
    "y_mask = tf.constant(nr_data_mask, dtype=tf.float64)\n",
    "\n",
    "discounts = tf.constant(discount_data, dtype=tf.float64)\n",
    "mixed_effect = tf.constant(macro_data, dtype=tf.float64)\n",
    "time_index = tf.constant(np.expand_dims(np.arange(1, macro_data.shape[0]+1), 1), dtype=tf.float64)\n",
    "shifted_nr = tf.constant(nr_shifted, dtype=tf.float64)\n",
    "y_vol = tf.constant(volume_data, dtype=tf.float64)\n",
    "\n",
    "val_splitter_ = tf.constant(5, dtype=tf.int32)\n",
    "val_splitter = 5 #if val_splitter_ == 5 else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "#Y\n",
    "y = tf.compat.v1.placeholder(dtype=tf.float64, name=\"nr_actual\")\n",
    "y_mask = tf.compat.v1.placeholder(dtype=tf.float64, name=\"nr_mask\")\n",
    "\n",
    "# X\n",
    "discounts = tf.compat.v1.placeholder(dtype=tf.float64, name=\"discounts\")\n",
    "mixed_effect = tf.compat.v1.placeholder(dtype=tf.float64, name=\"mixed_effects\")\n",
    "time_index = tf.compat.v1.placeholder(dtype=tf.float64, name=\"time_index\")\n",
    "# shifted_nr = tf.compat.v1.placeholder(dtype=tf.float64, name=\"shifted_nr\")\n",
    "y_vol = tf.compat.v1.placeholder(dtype=tf.float64, name=\"volume_actual\")\n",
    "\n",
    "val_splitter_ = tf.compat.v1.placeholder(dtype=tf.int32)\n",
    "val_splitter = 5 #if val_splitter_ == 5 else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_size = (1,nr_data.shape[1])\n",
    "me_size = macro_data.shape[-1]\n",
    "\n",
    "baseline_intercept = tf.Variable(np.expand_dims((nr_data.mean(0)*0.3), 0), dtype=tf.float64)\n",
    "\n",
    "baseline_slope1_global = tf.Variable(np.full((1,1), 0.1), dtype=tf.float64)\n",
    "baseline_slope1_hier = tf.Variable(np.full(dim_size, 0.1), dtype=tf.float64)\n",
    "baseline_slope1 = baseline_slope1_global + baseline_slope1_hier\n",
    "\n",
    "# baseline_slope2_global = tf.Variable(np.full((1,1), 0.1), dtype=tf.float64)\n",
    "# baseline_slope2_hier = tf.Variable(np.full(dim_size, 0.1), dtype=tf.float64)\n",
    "# baseline_slope2 = baseline_slope2_global + baseline_slope2_hier\n",
    "\n",
    "mixed_effect_mult_global = tf.Variable(np.random.normal(loc=0, size=(1, 1, me_size)), dtype=tf.float64)\n",
    "mixed_effect_mult_hier = tf.Variable(np.random.normal(loc=0, size=(*dim_size, me_size)), dtype=tf.float64)\n",
    "mixed_effect_mult = mixed_effect_mult_global + mixed_effect_mult_hier\n",
    "\n",
    "discount_slope_global = tf.math.sigmoid(tf.Variable(np.random.normal(loc=0, size=(1, 1, 2)), dtype=tf.float64))*3\n",
    "discount_slope_hier = tf.math.sigmoid(tf.Variable(np.random.normal(loc=0, size=(*dim_size, 2)), dtype=tf.float64))*3\n",
    "discount_slope = discount_slope_global + discount_slope_hier\n",
    "\n",
    "roi_mults_global = tf.Variable(np.random.normal(loc=0, size=(1, 1, me_size)), dtype=tf.float64)\n",
    "roi_mults_hier = tf.Variable(np.random.normal(loc=0, size=(*dim_size, me_size)), dtype=tf.float64)\n",
    "roi_mults = roi_mults_global + roi_mults_hier\n",
    "\n",
    "nr_to_vol_slope = tf.Variable(np.random.normal(loc=0, size=dim_size), dtype=tf.float64)\n",
    "\n",
    "\n",
    "hier_var_list = [baseline_slope1_hier, mixed_effect_mult_hier, discount_slope_hier, roi_mults_hier] #baseline_slope2_hier\n",
    "global_var_list = [baseline_slope1_global, mixed_effect_mult_global, discount_slope_global, roi_mults_global, nr_to_vol_slope] #baseline_slope2_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impacts\n",
    "base1 = tf.multiply(baseline_slope1, time_index) + baseline_intercept\n",
    "base2 = base1 #+ tf.multiply(baseline_slope2, shifted_nr)\n",
    "mixed_effect_impact = 1 + tf.nn.tanh(tf.multiply(mixed_effect, mixed_effect_mult))\n",
    "total_mixed_effect_impact = tf.reduce_prod(mixed_effect_impact, axis=-1)\n",
    "discount_impact = tf.multiply(discount_slope, discounts)\n",
    "roi_mult_impact = 1 + tf.nn.tanh(tf.multiply(mixed_effect_impact, roi_mults))\n",
    "total_roi_mult_impact = tf.expand_dims(tf.reduce_prod(roi_mult_impact, axis=1), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def wape(y_actual, y_prediction):\n",
    "    return tf.reduce_sum(tf.math.abs(y_actual - y_prediction))/tf.reduce_sum(y_actual)\n",
    "\n",
    "@tf.function\n",
    "def mse(y_actual, y_prediction):\n",
    "    return tf.reduce_sum(tf.math.square(y_actual - y_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "y_pred = tf.multiply(\n",
    "    y_mask,\n",
    "    (\n",
    "        tf.multiply(base2, total_mixed_effect_impact)\n",
    "        + tf.reduce_sum(discount_impact, axis=-1)\n",
    "    )\n",
    ")\n",
    "\n",
    "y_vol_pred = tf.multiply(y_pred, nr_to_vol_slope)\n",
    "\n",
    "y_split = tf.split(y, val_splitter)\n",
    "y_pred_split = tf.split(y_pred, val_splitter)\n",
    "\n",
    "y_vol_split = tf.split(y_vol, val_splitter)\n",
    "y_vol_pred_split = tf.split(y_vol_pred, val_splitter)\n",
    "\n",
    "\n",
    "# loss\n",
    "total_wape = tf.math.reduce_mean([wape(y_split[i], y_pred_split[i]) for i in range(0,val_splitter)])\n",
    "total_mse = mse(y, y_pred)\n",
    "actual_wape = wape(y, y_pred)\n",
    "\n",
    "total_wape_vol = tf.math.reduce_mean([wape(y_vol_split[i], y_vol_pred_split[i]) for i in range(0,val_splitter)])\n",
    "total_mse_vol = mse(y_vol, y_vol_pred)\n",
    "actual_wape_vol = wape(y_vol, y_vol_pred)\n",
    "\n",
    "\n",
    "reg1 = sum([tf.reduce_sum(tf.square(i)) for i in hier_var_list])\n",
    "reg2 = sum([tf.reduce_sum(tf.square(i)) for i in global_var_list])\n",
    "\n",
    "loss = (\n",
    "    1e3*total_wape_vol\n",
    "    +1e1*total_mse_vol\n",
    "    +1e3*total_wape\n",
    "    +1e1*total_mse\n",
    "    +1e3*reg2\n",
    "    +1e1*reg1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = 40\n",
    "\n",
    "feed_dict1 = {\n",
    "    discounts : discount_data[:splitter],\n",
    "    mixed_effect: macro_data[:splitter],\n",
    "    y_vol : volume_data[:splitter],\n",
    "    y : nr_data[:splitter],\n",
    "    # shifted_nr : nr_shifted[:splitter],\n",
    "    y_mask : nr_data_mask[:splitter],\n",
    "    time_index : time_index_array[:splitter],\n",
    "    val_splitter_ : 5\n",
    "}\n",
    "\n",
    "feed_dict2 = {\n",
    "    discounts : discount_data[splitter:-5],\n",
    "    mixed_effect: macro_data[splitter:-5],\n",
    "    y_vol : volume_data[splitter:-5],\n",
    "    y : nr_data[splitter:-5],\n",
    "    # shifted_nr : nr_shifted[splitter:-5],\n",
    "    y_mask : nr_data_mask[splitter:-5],\n",
    "    time_index : time_index_array[splitter:-5],\n",
    "    val_splitter_ : 5\n",
    "}\n",
    "\n",
    "feed_dict3 = {\n",
    "    discounts : discount_data[-2:],\n",
    "    mixed_effect: macro_data[-2:],\n",
    "    y_vol : volume_data[-2:],\n",
    "    y : nr_data[-2:],\n",
    "    # shifted_nr : nr_shifted[-2:],\n",
    "    y_mask : nr_data_mask[-2:],\n",
    "    time_index : time_index_array[-2:],\n",
    "    val_splitter_ : 5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "# optimizer\n",
    "lr = lambda x : 1 / np.power(x/5 + 10, 1/2)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=lr(epoch))#, beta1=0.1, beta2=0.1)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# initialize variables\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(init, feed_dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31622776601683794,\n",
       " 0.31311214554257477,\n",
       " 0.2886751345948129,\n",
       " 0.18257418583505536,\n",
       " 0.06900655593423542,\n",
       " 0.022304986837273527,\n",
       " 0.015791661046371634]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lr(i) for i in [0, 1, 10, 100, 1000, 10000, 20000]]#, 50000, 80000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/20000, Loss: 543049.9278, WAPE: 0.2030, WAPE_TEST: 0.3192, WAPE_VOL: 0.2052, WAPE_VOL_TEST: 0.2763, reg1: 1398.3324, reg2: 36.3997\n",
      "Epoch 500/20000, Loss: 534467.1720, WAPE: 0.2019, WAPE_TEST: 0.3048, WAPE_VOL: 0.2046, WAPE_VOL_TEST: 0.2712, reg1: 1431.1765, reg2: 35.4539\n",
      "Epoch 750/20000, Loss: 531670.2802, WAPE: 0.2018, WAPE_TEST: 0.3091, WAPE_VOL: 0.2042, WAPE_VOL_TEST: 0.2714, reg1: 1386.8642, reg2: 35.3025\n",
      "Epoch 1000/20000, Loss: 530612.9994, WAPE: 0.2011, WAPE_TEST: 0.3132, WAPE_VOL: 0.2041, WAPE_VOL_TEST: 0.2742, reg1: 1339.7920, reg2: 35.2373\n",
      "Epoch 1250/20000, Loss: 530132.5024, WAPE: 0.2011, WAPE_TEST: 0.3157, WAPE_VOL: 0.2043, WAPE_VOL_TEST: 0.2767, reg1: 1288.2510, reg2: 35.2193\n",
      "Epoch 1500/20000, Loss: 536561.1547, WAPE: 0.1981, WAPE_TEST: 0.3216, WAPE_VOL: 0.2080, WAPE_VOL_TEST: 0.2807, reg1: 1258.3257, reg2: 34.9466\n",
      "Epoch 1750/20000, Loss: 530135.2491, WAPE: 0.2039, WAPE_TEST: 0.3154, WAPE_VOL: 0.2040, WAPE_VOL_TEST: 0.2777, reg1: 1246.2881, reg2: 35.3746\n",
      "Epoch 2000/20000, Loss: 534361.9816, WAPE: 0.2082, WAPE_TEST: 0.3138, WAPE_VOL: 0.2043, WAPE_VOL_TEST: 0.2770, reg1: 1234.0758, reg2: 35.6658\n",
      "Epoch 2250/20000, Loss: 530400.7265, WAPE: 0.1996, WAPE_TEST: 0.3213, WAPE_VOL: 0.2047, WAPE_VOL_TEST: 0.2825, reg1: 1228.2937, reg2: 35.0886\n",
      "Epoch 2500/20000, Loss: 529958.0758, WAPE: 0.1997, WAPE_TEST: 0.3215, WAPE_VOL: 0.2046, WAPE_VOL_TEST: 0.2825, reg1: 1224.0504, reg2: 35.0979\n",
      "Epoch 2750/20000, Loss: 529727.6884, WAPE: 0.2020, WAPE_TEST: 0.3194, WAPE_VOL: 0.2040, WAPE_VOL_TEST: 0.2817, reg1: 1224.4845, reg2: 35.1711\n",
      "Epoch 3000/20000, Loss: 529617.9518, WAPE: 0.2011, WAPE_TEST: 0.3198, WAPE_VOL: 0.2041, WAPE_VOL_TEST: 0.2815, reg1: 1222.8408, reg2: 35.2386\n",
      "Epoch 3250/20000, Loss: 529917.0522, WAPE: 0.2001, WAPE_TEST: 0.3215, WAPE_VOL: 0.2044, WAPE_VOL_TEST: 0.2824, reg1: 1224.9414, reg2: 35.1322\n",
      "Epoch 3500/20000, Loss: 542966.2474, WAPE: 0.1977, WAPE_TEST: 0.3263, WAPE_VOL: 0.2113, WAPE_VOL_TEST: 0.2830, reg1: 1205.0142, reg2: 34.2247\n",
      "Epoch 3750/20000, Loss: 530409.6884, WAPE: 0.2041, WAPE_TEST: 0.3169, WAPE_VOL: 0.2038, WAPE_VOL_TEST: 0.2804, reg1: 1229.9258, reg2: 35.6021\n",
      "Epoch 4000/20000, Loss: 531918.8796, WAPE: 0.1978, WAPE_TEST: 0.3255, WAPE_VOL: 0.2060, WAPE_VOL_TEST: 0.2827, reg1: 1220.1461, reg2: 34.8945\n",
      "Epoch 4250/20000, Loss: 529669.1472, WAPE: 0.2010, WAPE_TEST: 0.3202, WAPE_VOL: 0.2041, WAPE_VOL_TEST: 0.2818, reg1: 1222.0686, reg2: 35.2375\n",
      "Epoch 4500/20000, Loss: 530073.7395, WAPE: 0.1998, WAPE_TEST: 0.3220, WAPE_VOL: 0.2047, WAPE_VOL_TEST: 0.2822, reg1: 1219.2755, reg2: 35.0388\n",
      "Epoch 4750/20000, Loss: 529712.1736, WAPE: 0.2004, WAPE_TEST: 0.3208, WAPE_VOL: 0.2044, WAPE_VOL_TEST: 0.2820, reg1: 1218.4233, reg2: 35.1841\n",
      "Epoch 5000/20000, Loss: 535536.6567, WAPE: 0.1967, WAPE_TEST: 0.3288, WAPE_VOL: 0.2087, WAPE_VOL_TEST: 0.2839, reg1: 1213.6261, reg2: 34.4258\n",
      "Epoch 5250/20000, Loss: 544154.9368, WAPE: 0.2138, WAPE_TEST: 0.3093, WAPE_VOL: 0.2070, WAPE_VOL_TEST: 0.2782, reg1: 1222.2921, reg2: 36.3102\n",
      "Epoch 5500/20000, Loss: 529653.4209, WAPE: 0.2012, WAPE_TEST: 0.3198, WAPE_VOL: 0.2040, WAPE_VOL_TEST: 0.2816, reg1: 1218.4777, reg2: 35.3148\n",
      "Epoch 5750/20000, Loss: 529632.0980, WAPE: 0.2011, WAPE_TEST: 0.3202, WAPE_VOL: 0.2041, WAPE_VOL_TEST: 0.2820, reg1: 1216.4761, reg2: 35.2725\n",
      "Epoch 6000/20000, Loss: 540311.9592, WAPE: 0.2112, WAPE_TEST: 0.3118, WAPE_VOL: 0.2064, WAPE_VOL_TEST: 0.2796, reg1: 1225.0521, reg2: 36.1507\n",
      "Epoch 6250/20000, Loss: 530125.5546, WAPE: 0.2027, WAPE_TEST: 0.3162, WAPE_VOL: 0.2041, WAPE_VOL_TEST: 0.2804, reg1: 1224.8078, reg2: 35.5185\n",
      "Epoch 6500/20000, Loss: 530085.3133, WAPE: 0.2005, WAPE_TEST: 0.3196, WAPE_VOL: 0.2044, WAPE_VOL_TEST: 0.2811, reg1: 1224.3853, reg2: 35.1972\n",
      "Epoch 6750/20000, Loss: 532599.0495, WAPE: 0.1990, WAPE_TEST: 0.3220, WAPE_VOL: 0.2051, WAPE_VOL_TEST: 0.2823, reg1: 1237.4367, reg2: 35.2778\n",
      "Epoch 7000/20000, Loss: 537962.4705, WAPE: 0.1964, WAPE_TEST: 0.3293, WAPE_VOL: 0.2099, WAPE_VOL_TEST: 0.2846, reg1: 1213.7601, reg2: 34.3591\n",
      "Epoch 7250/20000, Loss: 531084.3905, WAPE: 0.1991, WAPE_TEST: 0.3222, WAPE_VOL: 0.2047, WAPE_VOL_TEST: 0.2820, reg1: 1218.6312, reg2: 35.0722\n",
      "Epoch 7500/20000, Loss: 530381.8725, WAPE: 0.2038, WAPE_TEST: 0.3161, WAPE_VOL: 0.2040, WAPE_VOL_TEST: 0.2807, reg1: 1231.7101, reg2: 35.4591\n",
      "Epoch 7750/20000, Loss: 529992.1217, WAPE: 0.2028, WAPE_TEST: 0.3194, WAPE_VOL: 0.2037, WAPE_VOL_TEST: 0.2821, reg1: 1220.6222, reg2: 35.5580\n",
      "Epoch 8000/20000, Loss: 529961.2318, WAPE: 0.2023, WAPE_TEST: 0.3186, WAPE_VOL: 0.2038, WAPE_VOL_TEST: 0.2816, reg1: 1224.4491, reg2: 35.4285\n",
      "Epoch 8250/20000, Loss: 529782.9066, WAPE: 0.2021, WAPE_TEST: 0.3190, WAPE_VOL: 0.2039, WAPE_VOL_TEST: 0.2811, reg1: 1207.4053, reg2: 35.3013\n",
      "Epoch 8500/20000, Loss: 531687.6075, WAPE: 0.2055, WAPE_TEST: 0.3154, WAPE_VOL: 0.2043, WAPE_VOL_TEST: 0.2798, reg1: 1230.3412, reg2: 35.6425\n",
      "Epoch 8750/20000, Loss: 531332.3243, WAPE: 0.2054, WAPE_TEST: 0.3157, WAPE_VOL: 0.2043, WAPE_VOL_TEST: 0.2793, reg1: 1230.8734, reg2: 35.5859\n",
      "Epoch 9000/20000, Loss: 534544.7497, WAPE: 0.1989, WAPE_TEST: 0.3230, WAPE_VOL: 0.2068, WAPE_VOL_TEST: 0.2812, reg1: 1222.6433, reg2: 34.7094\n",
      "Epoch 9250/20000, Loss: 529728.5619, WAPE: 0.2005, WAPE_TEST: 0.3208, WAPE_VOL: 0.2043, WAPE_VOL_TEST: 0.2816, reg1: 1223.2277, reg2: 35.1565\n",
      "Epoch 9500/20000, Loss: 531327.1375, WAPE: 0.2036, WAPE_TEST: 0.3189, WAPE_VOL: 0.2047, WAPE_VOL_TEST: 0.2816, reg1: 1226.2212, reg2: 35.4650\n",
      "Epoch 9750/20000, Loss: 530203.7669, WAPE: 0.2015, WAPE_TEST: 0.3200, WAPE_VOL: 0.2043, WAPE_VOL_TEST: 0.2826, reg1: 1217.9233, reg2: 35.3883\n",
      "Epoch 10000/20000, Loss: 543464.3626, WAPE: 0.1969, WAPE_TEST: 0.3253, WAPE_VOL: 0.2100, WAPE_VOL_TEST: 0.2845, reg1: 1225.4377, reg2: 34.5450\n",
      "Epoch 10250/20000, Loss: 532522.8406, WAPE: 0.2056, WAPE_TEST: 0.3174, WAPE_VOL: 0.2047, WAPE_VOL_TEST: 0.2802, reg1: 1225.2131, reg2: 35.5871\n",
      "Epoch 10500/20000, Loss: 529678.7537, WAPE: 0.2010, WAPE_TEST: 0.3210, WAPE_VOL: 0.2040, WAPE_VOL_TEST: 0.2821, reg1: 1221.5956, reg2: 35.2837\n",
      "Epoch 10750/20000, Loss: 531822.9522, WAPE: 0.2051, WAPE_TEST: 0.3160, WAPE_VOL: 0.2043, WAPE_VOL_TEST: 0.2798, reg1: 1225.9240, reg2: 35.5875\n",
      "Epoch 11000/20000, Loss: 535382.5714, WAPE: 0.1966, WAPE_TEST: 0.3291, WAPE_VOL: 0.2087, WAPE_VOL_TEST: 0.2843, reg1: 1214.5889, reg2: 34.4953\n",
      "Epoch 11250/20000, Loss: 539437.1136, WAPE: 0.1980, WAPE_TEST: 0.3269, WAPE_VOL: 0.2104, WAPE_VOL_TEST: 0.2827, reg1: 1217.3711, reg2: 34.2540\n",
      "Epoch 11500/20000, Loss: 534544.8793, WAPE: 0.1981, WAPE_TEST: 0.3241, WAPE_VOL: 0.2073, WAPE_VOL_TEST: 0.2817, reg1: 1217.4576, reg2: 34.5708\n",
      "Epoch 11750/20000, Loss: 530517.0778, WAPE: 0.2014, WAPE_TEST: 0.3204, WAPE_VOL: 0.2048, WAPE_VOL_TEST: 0.2808, reg1: 1195.3071, reg2: 35.2373\n",
      "Epoch 12000/20000, Loss: 529995.4221, WAPE: 0.2027, WAPE_TEST: 0.3181, WAPE_VOL: 0.2035, WAPE_VOL_TEST: 0.2820, reg1: 1219.9882, reg2: 35.4482\n",
      "Epoch 12250/20000, Loss: 529843.6235, WAPE: 0.2025, WAPE_TEST: 0.3196, WAPE_VOL: 0.2039, WAPE_VOL_TEST: 0.2827, reg1: 1220.5837, reg2: 35.4173\n",
      "Epoch 12500/20000, Loss: 536653.3037, WAPE: 0.2027, WAPE_TEST: 0.3185, WAPE_VOL: 0.2068, WAPE_VOL_TEST: 0.2843, reg1: 1239.4592, reg2: 35.6329\n",
      "Epoch 12750/20000, Loss: 529778.4941, WAPE: 0.2017, WAPE_TEST: 0.3197, WAPE_VOL: 0.2039, WAPE_VOL_TEST: 0.2824, reg1: 1224.5292, reg2: 35.3131\n",
      "Epoch 13000/20000, Loss: 531477.5982, WAPE: 0.2056, WAPE_TEST: 0.3158, WAPE_VOL: 0.2036, WAPE_VOL_TEST: 0.2801, reg1: 1218.2631, reg2: 35.6258\n",
      "Epoch 13250/20000, Loss: 530350.8993, WAPE: 0.2006, WAPE_TEST: 0.3198, WAPE_VOL: 0.2045, WAPE_VOL_TEST: 0.2822, reg1: 1226.9134, reg2: 35.1055\n",
      "Epoch 13500/20000, Loss: 529889.7293, WAPE: 0.2020, WAPE_TEST: 0.3204, WAPE_VOL: 0.2041, WAPE_VOL_TEST: 0.2818, reg1: 1221.8873, reg2: 35.3103\n",
      "Epoch 13750/20000, Loss: 537119.1600, WAPE: 0.2078, WAPE_TEST: 0.3140, WAPE_VOL: 0.2060, WAPE_VOL_TEST: 0.2763, reg1: 1234.3778, reg2: 36.1243\n",
      "Epoch 14000/20000, Loss: 529999.3253, WAPE: 0.2022, WAPE_TEST: 0.3184, WAPE_VOL: 0.2045, WAPE_VOL_TEST: 0.2810, reg1: 1222.3378, reg2: 35.2469\n",
      "Epoch 14250/20000, Loss: 530242.0468, WAPE: 0.2019, WAPE_TEST: 0.3194, WAPE_VOL: 0.2041, WAPE_VOL_TEST: 0.2812, reg1: 1226.7090, reg2: 35.2917\n",
      "Epoch 14500/20000, Loss: 536252.5254, WAPE: 0.2080, WAPE_TEST: 0.3165, WAPE_VOL: 0.2053, WAPE_VOL_TEST: 0.2802, reg1: 1227.9594, reg2: 35.7404\n",
      "Epoch 14750/20000, Loss: 530108.8243, WAPE: 0.1999, WAPE_TEST: 0.3222, WAPE_VOL: 0.2046, WAPE_VOL_TEST: 0.2824, reg1: 1225.0457, reg2: 35.1725\n",
      "Epoch 15000/20000, Loss: 530175.6391, WAPE: 0.2031, WAPE_TEST: 0.3184, WAPE_VOL: 0.2036, WAPE_VOL_TEST: 0.2801, reg1: 1230.4156, reg2: 35.7451\n",
      "Epoch 15250/20000, Loss: 529924.3434, WAPE: 0.2008, WAPE_TEST: 0.3223, WAPE_VOL: 0.2047, WAPE_VOL_TEST: 0.2831, reg1: 1220.7043, reg2: 35.1028\n",
      "Epoch 15500/20000, Loss: 529661.3065, WAPE: 0.2007, WAPE_TEST: 0.3201, WAPE_VOL: 0.2041, WAPE_VOL_TEST: 0.2818, reg1: 1221.9413, reg2: 35.2189\n",
      "Epoch 15750/20000, Loss: 532949.5484, WAPE: 0.2056, WAPE_TEST: 0.3211, WAPE_VOL: 0.2052, WAPE_VOL_TEST: 0.2815, reg1: 1231.2334, reg2: 35.4870\n",
      "Epoch 16000/20000, Loss: 530304.9465, WAPE: 0.2006, WAPE_TEST: 0.3196, WAPE_VOL: 0.2044, WAPE_VOL_TEST: 0.2815, reg1: 1225.3006, reg2: 35.2435\n",
      "Epoch 16250/20000, Loss: 534160.5662, WAPE: 0.1976, WAPE_TEST: 0.3279, WAPE_VOL: 0.2083, WAPE_VOL_TEST: 0.2836, reg1: 1217.2930, reg2: 34.4358\n",
      "Epoch 16500/20000, Loss: 529744.9588, WAPE: 0.2016, WAPE_TEST: 0.3197, WAPE_VOL: 0.2038, WAPE_VOL_TEST: 0.2820, reg1: 1220.7872, reg2: 35.3848\n",
      "Epoch 16750/20000, Loss: 531311.0156, WAPE: 0.1994, WAPE_TEST: 0.3197, WAPE_VOL: 0.2047, WAPE_VOL_TEST: 0.2802, reg1: 1231.6999, reg2: 35.3089\n",
      "Epoch 17000/20000, Loss: 535492.1284, WAPE: 0.1974, WAPE_TEST: 0.3249, WAPE_VOL: 0.2064, WAPE_VOL_TEST: 0.2822, reg1: 1223.2744, reg2: 34.9986\n",
      "Epoch 17250/20000, Loss: 529809.7288, WAPE: 0.2006, WAPE_TEST: 0.3199, WAPE_VOL: 0.2043, WAPE_VOL_TEST: 0.2802, reg1: 1222.8086, reg2: 35.2669\n",
      "Epoch 17500/20000, Loss: 529792.6640, WAPE: 0.2020, WAPE_TEST: 0.3167, WAPE_VOL: 0.2040, WAPE_VOL_TEST: 0.2799, reg1: 1226.1747, reg2: 35.4434\n",
      "Epoch 17750/20000, Loss: 530400.1520, WAPE: 0.2039, WAPE_TEST: 0.3197, WAPE_VOL: 0.2036, WAPE_VOL_TEST: 0.2813, reg1: 1232.1448, reg2: 35.5470\n",
      "Epoch 18000/20000, Loss: 529899.4269, WAPE: 0.2010, WAPE_TEST: 0.3211, WAPE_VOL: 0.2040, WAPE_VOL_TEST: 0.2823, reg1: 1219.1237, reg2: 35.3262\n",
      "Epoch 18250/20000, Loss: 530102.2841, WAPE: 0.1998, WAPE_TEST: 0.3219, WAPE_VOL: 0.2046, WAPE_VOL_TEST: 0.2824, reg1: 1225.5065, reg2: 35.1570\n",
      "Epoch 18500/20000, Loss: 529902.5785, WAPE: 0.2030, WAPE_TEST: 0.3178, WAPE_VOL: 0.2037, WAPE_VOL_TEST: 0.2815, reg1: 1223.6336, reg2: 35.4768\n",
      "Epoch 18750/20000, Loss: 530181.4103, WAPE: 0.2018, WAPE_TEST: 0.3197, WAPE_VOL: 0.2040, WAPE_VOL_TEST: 0.2808, reg1: 1218.9634, reg2: 35.3021\n",
      "Epoch 19000/20000, Loss: 539178.6624, WAPE: 0.1963, WAPE_TEST: 0.3277, WAPE_VOL: 0.2095, WAPE_VOL_TEST: 0.2842, reg1: 1219.2743, reg2: 34.4978\n",
      "Epoch 19250/20000, Loss: 531118.8761, WAPE: 0.2053, WAPE_TEST: 0.3164, WAPE_VOL: 0.2038, WAPE_VOL_TEST: 0.2810, reg1: 1212.9129, reg2: 35.6719\n",
      "Epoch 19500/20000, Loss: 529815.8212, WAPE: 0.2002, WAPE_TEST: 0.3195, WAPE_VOL: 0.2041, WAPE_VOL_TEST: 0.2822, reg1: 1227.7711, reg2: 35.2383\n",
      "Epoch 19750/20000, Loss: 530334.6447, WAPE: 0.2049, WAPE_TEST: 0.3181, WAPE_VOL: 0.2037, WAPE_VOL_TEST: 0.2808, reg1: 1222.0389, reg2: 35.6658\n",
      "Epoch 20000/20000, Loss: 533204.4221, WAPE: 0.2071, WAPE_TEST: 0.3160, WAPE_VOL: 0.2051, WAPE_VOL_TEST: 0.2794, reg1: 1228.8732, reg2: 35.6424\n"
     ]
    }
   ],
   "source": [
    "metric_update_track = {\n",
    "    \"epoch\" : [],\n",
    "    \"actual_wape\" : [],\n",
    "    \"test_wape\" : [],\n",
    "    \"loss\" : [],\n",
    "    \"mse\" : [],\n",
    "    \"reg1\" : [],\n",
    "    \"reg2\" : []\n",
    "}\n",
    "\n",
    "# train model\n",
    "num_epochs = 20000\n",
    "for epoch in range(num_epochs):\n",
    "    (\n",
    "        _,\n",
    "        current_loss,\n",
    "        current_wape,\n",
    "        # current_mse,\n",
    "        current_wape_vol,\n",
    "        # current_mse_vol,\n",
    "        current_reg1,\n",
    "        current_reg2\n",
    "    )= sess.run([\n",
    "        train,\n",
    "        loss,\n",
    "        actual_wape,\n",
    "        # total_mse,\n",
    "        actual_wape_vol,\n",
    "        # total_mse_vol,\n",
    "        reg1,\n",
    "        reg2\n",
    "    ], feed_dict1)\n",
    "\n",
    "    current_wape_test, current_wape_vol_test = sess.run([actual_wape, actual_wape_vol], feed_dict2)\n",
    "\n",
    "\n",
    "    if (epoch + 1) % 250 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {current_loss:.4f}, WAPE: {current_wape:.4f}, WAPE_TEST: {current_wape_test:.4f}, WAPE_VOL: {current_wape_vol:.4f}, WAPE_VOL_TEST: {current_wape_vol_test:.4f}, reg1: {current_reg1:.4f}, reg2: {current_reg2:.4f}\")\n",
    "        # metric_update_track[\"epoch\"].append(epoch)\n",
    "        # metric_update_track[\"actual_wape\"].append(current_wape)\n",
    "        # metric_update_track[\"test_wape\"].append(current_wape_test)\n",
    "        # metric_update_track[\"loss\"].append(current_loss)\n",
    "        # metric_update_track[\"mse\"].append(current_mse)\n",
    "        # metric_update_track[\"reg1\"].append(current_reg1)\n",
    "        # metric_update_track[\"reg2\"].append(current_reg2)\n",
    "\n",
    "\n",
    "\n",
    "#         # Training loop\n",
    "# num_epochs = 500\n",
    "# for epoch in range(num_epochs):\n",
    "#     _, current_error, cuurent_mse, current_m1, current_m2, current_c = sess.run([train_op, error, mse_error, m1, m2, c])\n",
    "#     if (epoch + 1) % 25 == 0:\n",
    "#         print(f\"Epoch {epoch + 1}/{num_epochs}, Error: {current_error:.4f}, MSE: {cuurent_mse:.4f}, m1: {current_m1}, m2: {current_m2}, c: {current_c}\")\n",
    "\n",
    "# # Print the final results for 'm' and 'c'\n",
    "# final_m1, final_m2, final_c = sess.run([m1, m2, c])\n",
    "# print(f\"Final 'm1' value: {final_m1}\")\n",
    "\n",
    "# print(f\"Final 'm2' value: {final_m2}\")\n",
    "# print(f\"Final 'c' value: {final_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_submit, vol_submit = sess.run([y_pred, y_vol_pred], feed_dict3)\n",
    "nr_submit = nr_submit * scaler\n",
    "vol_submit = vol_submit * vol_scaler\n",
    "\n",
    "nr_data_temp = (\n",
    "    sales_data\n",
    "    .reset_index()\n",
    "    .groupby([\"date\", \"sku\"])\n",
    "    .net_revenue.sum()\n",
    "    .sort_index()\n",
    "    .unstack(1)\n",
    ")\n",
    "nr_submit = pd.DataFrame(nr_submit, index=nr_data_temp.index[-2:], columns=nr_data_temp.columns)\n",
    "vol_submit = pd.DataFrame(vol_submit, index=nr_data_temp.index[-2:], columns=nr_data_temp.columns)\n",
    "\n",
    "submit_temp = sales_data[sales_data.gto.isna()].reset_index().set_index([\"date\", \"sku\", \"brand\", \"pack\", \"size\"]).sort_index()\n",
    "submit_temp.loc[:, \"net_revenue\"] = submit_temp.net_revenue.fillna(nr_submit.stack()).apply(lambda x: x if x>0 else -x/2)\n",
    "submit_temp.loc[:, \"volume\"] = submit_temp.volume.fillna(vol_submit.stack()).apply(lambda x: x if x>0 else -x/2)\n",
    "submit_temp = submit_temp.reset_index()\n",
    "\n",
    "cols_req = [ \"Year\", \"Month\", \"SKU\", \"Brand\", \"Pack\", \"Size\", \"Volume_Estimate\", \"Net_Revenue_Estimate\", \"Optimal_Promotional_Discount\", \"Optimal_Other_Discounts\", \"Optimal_Volume\", \"Optimal_Net_Revenue\"]\n",
    "\n",
    "\n",
    "submit_temp.loc[:, \"Year\"] = submit_temp.date.dt.year\n",
    "submit_temp.loc[:, \"Month\"] = submit_temp.date.dt.month\n",
    "submit_temp.loc[:, \"SKU\"] = submit_temp.sku\n",
    "submit_temp.loc[:, \"Brand\"] = submit_temp.brand\n",
    "submit_temp.loc[:, \"Pack\"] = submit_temp.pack\n",
    "submit_temp.loc[:, \"Size\"] = submit_temp.size\n",
    "submit_temp.loc[:, \"Volume_Estimate\"] = submit_temp.volume\n",
    "submit_temp.loc[:, \"Net_Revenue_Estimate\"] = submit_temp.net_revenue\n",
    "submit_temp.loc[:, \"Optimal_Promotional_Discount\"] = submit_temp.promotional_discount\n",
    "submit_temp.loc[:, \"Optimal_Other_Discounts\"] = submit_temp.other_discounts\n",
    "submit_temp.loc[:, \"Optimal_Volume\"] = submit_temp.volume\n",
    "submit_temp.loc[:, \"Optimal_Net_Revenue\"] = submit_temp.net_revenue\n",
    "\n",
    "submit_temp = submit_temp[cols_req]\n",
    "submit_temp.to_csv(\"/home/akshay-development-server/promo-optimization_team-simpsons-paradox/data/team_simpsons_paradox_submission_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mroi_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
