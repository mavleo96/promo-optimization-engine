{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install azure-storage-blob\n",
    "# !pip install python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from setup_utils import fetch_data, load_data, create_time_index\n",
    "\n",
    "CONNECTION_STRING = os.getenv(\"CONNECTION_STRING\")\n",
    "\n",
    "load_dotenv()\n",
    "# fetch_data(CONNECTION_STRING)\n",
    "\n",
    "(\n",
    "    brand_mapping,\n",
    "    macro_data,\n",
    "    brand_constraint,\n",
    "    pack_constraint,\n",
    "    segment_constraint,\n",
    "    sales_data,\n",
    "    volume_variation_constraint,\n",
    ") = load_data()\n",
    "\n",
    "(\n",
    "    macro_data,\n",
    "    sales_data,\n",
    ") = create_time_index([macro_data, sales_data])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = sales_data[sales_data.volume.notna()].join(macro_data, on=\"date\", how=\"left\")\n",
    "# df_test = sales_data[sales_data.volume.isna()].join(macro_data, on=\"date\", how=\"left\")\n",
    "\n",
    "df = sales_data.join(macro_data, on=\"date\", how=\"left\")\n",
    "df = df[((df.gto>0) & (df.volume>0) & (df.promotional_discount<=0) & (df.other_discounts<=0)) | df.gto.isna() ].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = sales_data[sales_data.gto.isna()].reset_index()\n",
    "temp_data[\"month\"] = temp_data.date.dt.month\n",
    "temp_data[\"year\"] = temp_data.date.dt.year\n",
    "temp_data = temp_data.fillna(10000)\n",
    "temp_data = temp_data.merge(brand_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_mapping = temp_data[[\"sku\", \"pack\", \"brand\", \"segment\"]].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_dict = {\n",
    "    \"brand\" : brand_constraint,\n",
    "    \"pack\" : pack_constraint,\n",
    "    \"segment\" : segment_constraint,\n",
    "    \"volume_variation\" : volume_variation_constraint\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "\n",
    "    def __init__(self, master_mapping, constraint_dict):\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        self.sess = tf.compat.v1.Session()\n",
    "\n",
    "        n_time_period = 2\n",
    "        n_sku = 151\n",
    "        n_vehicle = 2\n",
    "\n",
    "        x_shape = (n_time_period, n_sku, n_vehicle)\n",
    "        self.x_initial = np.random.normal(loc=100, scale=10, size=x_shape)\n",
    "\n",
    "        self.master_mapping = master_mapping\n",
    "        self._encodings = self._create_encodings(master_mapping)\n",
    "\n",
    "        self.slopes = tf.constant(np.random.rand(2,151,2), name='x', dtype=tf.float64)\n",
    "\n",
    "        self.constraint_dict = constraint_dict\n",
    "        self._brand_constraint_tensor = self._constraint_tensor_generate(self.constraint_dict[\"brand\"], self._encodings, \"brand\")\n",
    "        self._pack_constraint_tensor = self._constraint_tensor_generate(self.constraint_dict[\"pack\"], self._encodings, \"pack\")\n",
    "        self._segment_constraint_tensor = self._constraint_tensor_generate(self.constraint_dict[\"segment\"], self._encodings, \"segment\")\n",
    "\n",
    "        self.x = tf.Variable(np.random.normal(loc=100, scale=10, size=x_shape), name='x', dtype=tf.float64)\n",
    "\n",
    "        self.x_brand = self._tensor_gather(self.x, self._encodings, \"brand\")\n",
    "        self.x_pack = self._tensor_gather(self.x, self._encodings, \"pack\")\n",
    "        self.x_segment = self._tensor_gather(self.x, self._encodings, \"segment\")\n",
    "\n",
    "        self.brand_constraint_loss = tf.reduce_sum(tf.nn.relu(self._brand_constraint_tensor - self.x_brand))\n",
    "        self.pack_constraint_loss = tf.reduce_sum(tf.nn.relu(self._pack_constraint_tensor - self.x_pack))\n",
    "        self.segment_constraint_loss = tf.reduce_sum(tf.nn.relu(self._segment_constraint_tensor - self.x_segment))\n",
    "        \n",
    "        self.y_pred_opt = self.fake_model(self.x)\n",
    "        self.y_pred_cons = self.fake_model(self.x_initial)\n",
    "\n",
    "        self.roi = self._calculate_roi(self.y_pred_opt, self.y_pred_cons, self.x)\n",
    "\n",
    "        self.loss = self.define_loss()\n",
    "\n",
    "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.1)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    @tf.function\n",
    "    def fake_model(self, X):\n",
    "        return tf.reduce_sum(tf.multiply(self.slopes, tf.pow(tf.nn.relu(X), tf.constant(0.75, dtype=tf.float64))))\n",
    "\n",
    "    # @tf.function\n",
    "    # def fake_model_cons(self, X):\n",
    "    #     return tf.reduce_sum(tf.multiply(self.slopes, X))\n",
    "\n",
    "    def _create_encodings(self, master_map):\n",
    "\n",
    "        def label_encoder(series):\n",
    "            unique_values = series.sort_values().unique()\n",
    "            unique_count =  series.nunique()\n",
    "\n",
    "            return dict(zip(unique_values, range(len(unique_values))))\n",
    "        \n",
    "        def mapper(col_val, col_key=\"sku\"):\n",
    "\n",
    "            df = master_mapping[[col_key, col_val]].drop_duplicates()\n",
    "            df.loc[:,col_val] = df[col_val].map(label_dict[col_val])\n",
    "            df.loc[:,col_key] = df[col_key].map(label_dict[col_key])\n",
    "\n",
    "            return df.set_index(col_key).to_dict()[col_val]\n",
    "\n",
    "        label_dict = {col:label_encoder(master_map[col]) for col in master_map.columns}\n",
    "        mapper_dict = {col:mapper(col) for col in master_map.columns if col!=\"sku\"}\n",
    "\n",
    "        return {\"label_dict\" : label_dict, \"mapper_dict\" : mapper_dict}\n",
    "\n",
    "    @tf.function\n",
    "    def _calculate_roi(self, y_opt, y_act, x):\n",
    "        return -tf.divide(tf.reduce_sum(tf.subtract(self.y_pred_opt, self.y_pred_cons)), tf.reduce_sum(self.x))\n",
    "\n",
    "    @tf.function\n",
    "    def _tensor_gather(self, x, encoding, key):\n",
    "        encoding = pd.Series(encoding[\"mapper_dict\"][key]).sort_index().to_numpy()\n",
    "        segment_ids = tf.constant(encoding, dtype=tf.int32)\n",
    "        x_transpose = tf.transpose(x, perm=[1,0,2])\n",
    "        x_gathered = tf.math.unsorted_segment_sum(x_transpose, segment_ids, num_segments=encoding.max()+1)\n",
    "        x_gathered_transpose = tf.reduce_sum(tf.transpose(x_gathered, perm=[1,0,2]), axis=2)\n",
    "\n",
    "        return x_gathered_transpose\n",
    "\n",
    "    def _constraint_tensor_generate(self, constraint, encoding, key):\n",
    "\n",
    "        encoding_length = max(encoding[\"label_dict\"][key].values())+1\n",
    "        constraint = constraint.copy(deep=True)\n",
    "        constraint = constraint.replace(encoding[\"label_dict\"][key]).sort_values([\"month\", key])\n",
    "        constraint = constraint.groupby([\"month\", key]).max_discount.sum().sort_index().unstack(1)\n",
    "\n",
    "        constraint = pd.DataFrame(columns=pd.Index(range(0,encoding_length), dtype='int64', name=\"brand\"), index=pd.Index(range(6,8), dtype='int64')).fillna(constraint).fillna(0.0).to_numpy()\n",
    "\n",
    "        return constraint\n",
    "    \n",
    "    @tf.function\n",
    "    def define_loss(self):\n",
    "        return -self.roi + tf.nn.relu(tf.reduce_sum(self.x) - self.x_initial.sum()) #+ self.brand_constraint_loss + self.pack_constraint_loss + self.segment_constraint_loss\n",
    "    \n",
    "        \n",
    "\n",
    "    def train(self, epochs):\n",
    "        for i in range(epochs):\n",
    "            _, loss = self.sess.run([self.train_op, self.loss])\n",
    "            # self.roi = self._calculate_roi(self.y_pred_opt, self.y_pred_cons, self.x)\n",
    "            if i%100==0:\n",
    "                print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(master_mapping, constraints_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.train(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.x_initial.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.sess.run(opt.x)#.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.sess.run(opt.roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def fake_model(X):\n",
    "    return tf.multiply(tf.constant(np.random.normal(loc=2, scale=0.1, size=None), dtype=tf.float64), tf.reduce_sum(X, axis=2), name='volume_optimal')\n",
    "\n",
    "@tf.function\n",
    "def fake_model_cons(X):\n",
    "    return tf.reduce_sum(X, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "print(sess.run(objective))\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    _, current_error, current_x = sess.run([train_op, objective, x])\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Error: {sess.run(objective):.4f}\")\n",
    "\n",
    "# Print the final results for 'm' and 'c'\n",
    "# final_m, final_c = sess.run(x)\n",
    "# print(f\"Final 'm' value: {final_m:.4f}\")\n",
    "# print(f\"Final 'c' value: {final_c:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# # Example data\n",
    "# data = tf.constant([2, 3, 1, 5, 2, 4], dtype=tf.float32)\n",
    "# segment_ids = tf.constant([0, 1, 0, 1, 2, 2], dtype=tf.int32)  # Unsorted segment IDs\n",
    "# sorted_segment_indices = tf.argsort(segment_ids)\n",
    "# sorted_segment_ids = tf.gather(segment_ids, sorted_segment_indices)\n",
    "\n",
    "# # Create unique segment IDs for sorting\n",
    "# unique_segment_ids, _ = tf.unique(segment_ids)\n",
    "\n",
    "# # Sort the data based on the sorted segment IDs\n",
    "# sorted_data = tf.gather(data, tf.argsort(segment_ids))\n",
    "\n",
    "# # Compute the segment sum using tf.segment_sum with sorted data and original segment IDs\n",
    "# segment_sum_result = tf.math.segment_sum(sorted_data, sorted_segment_ids)\n",
    "\n",
    "# # Create a TensorFlow session\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # Calculate the segment sum\n",
    "    result = sess.run(qq)\n",
    "\n",
    "    # # Print the result\n",
    "    # print(\"Data:\")\n",
    "    # print(sess.run(data))\n",
    "    # print(\"Segment IDs:\")\n",
    "    # print(sess.run(segment_ids))\n",
    "    # print(\"Segment Sum Result:\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_og = df.copy(deep=True)\n",
    "\n",
    "# # Identify and convert categorical columns to label encoding\n",
    "# categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "# label_encoders = {}\n",
    "# for col in categorical_columns:\n",
    "#     le = LabelEncoder()\n",
    "#     df[col] = le.fit_transform(df[col])\n",
    "#     label_encoders[col] = le\n",
    "\n",
    "\n",
    "# target_column = [\"gto\", \"volume\"]\n",
    "# X = df[df.gto.notna()].drop(columns=[*target_column])\n",
    "# y = df[df.gto.notna()][target_column]\n",
    "\n",
    "# X_test = df[df.gto.isna()].drop(columns=[*target_column])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Determine the number of levels by finding the maximum index along each dimension\n",
    "# num_levels = max(len(index) for index in series.index)\n",
    "\n",
    "# # Initialize an empty multi-dimensional array with zeros\n",
    "# shape = tuple(np.max(series.index, axis=0) + 1)  # Determine the shape based on the maximum indices\n",
    "# np.array(series).reshape(shape=shape)\n",
    "\n",
    "\n",
    "# def create_multi_dim_array(series):\n",
    "#     \"create a multi dimensional array from multdimensional index series according to number of levels in series\"\n",
    "    \n",
    "#     num_levels = max(len(index) for index in series.index)\n",
    "\n",
    "#     # Initialize an empty multi-dimensional array with zeros\n",
    "#     shape = [series.index.get_level_values(i).nunique() for i in range(num_levels)]\n",
    "    \n",
    "#     return np.array(series).reshape(shape)\n",
    "\n",
    "# create_multi_dim_array(series)\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from itertools import product\n",
    "\n",
    "# def create_multi_dim_array(series):\n",
    "#     \"\"\"Create a multi-dimensional array from a multi-dimensional index series\n",
    "#     by first creating the Cartesian product of index levels and then reshaping.\"\"\"\n",
    "    \n",
    "#     # Calculate the number of levels and the unique values in each level\n",
    "#     num_levels = len(series.index.levels)\n",
    "#     unique_values = [level.unique() for level in series.index.levels]\n",
    "    \n",
    "#     # Create the Cartesian product of index levels\n",
    "#     cartesian_product = list(product(*unique_values))\n",
    "    \n",
    "#     # Create a new DataFrame with the Cartesian product as the index\n",
    "#     merged_index = pd.MultiIndex.from_tuples(cartesian_product, names=series.index.names)\n",
    "#     merged_series = pd.Series(0, index=merged_index, dtype=int)  # Initialize with zeros\n",
    "    \n",
    "#     # Merge the original series into the new DataFrame\n",
    "#     merged_series.update(series)\n",
    "    \n",
    "#     # Reshape the merged series into a multi-dimensional NumPy array\n",
    "#     shape = [len(level) for level in merged_series.index.levels]\n",
    "#     multi_dim_array = np.array(merged_series).reshape(shape)\n",
    "    \n",
    "#     return multi_dim_array, unique_values\n",
    "\n",
    "# create_multi_dim_array(series)[1]\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# # Determine the number of levels by finding the maximum index along each dimension\n",
    "# num_levels = max(len(index) for index in series.index)\n",
    "\n",
    "# # Initialize an empty multi-dimensional array with zeros\n",
    "# shape = [series.index.get_level_values(i).nunique() for i in range(num_levels)]\n",
    "# multi_dim_array = np.zeros(shape, dtype=float)\n",
    "\n",
    "# # # Fill the array using the index series\n",
    "# # for index in series.index:\n",
    "# #     multi_dim_array[index] = 1  # You can set any value you want here\n",
    "\n",
    "# # print(multi_dim_array)\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
