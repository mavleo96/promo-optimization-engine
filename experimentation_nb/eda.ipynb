{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install azure-storage-blob\n",
    "# !pip install python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from setup_utils import fetch_data, load_data, create_time_index\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "CONNECTION_STRING = os.getenv(\"CONNECTION_STRING\")\n",
    "\n",
    "load_dotenv()\n",
    "fetch_data(CONNECTION_STRING)\n",
    "\n",
    "(\n",
    "    brand_mapping,\n",
    "    macro_data,\n",
    "    brand_constraint,\n",
    "    pack_constraint,\n",
    "    segment_constraint,\n",
    "    sales_data,\n",
    "    volume_variation_constraint,\n",
    ") = load_data()\n",
    "\n",
    "(\n",
    "    macro_data,\n",
    "    sales_data,\n",
    ") = create_time_index([macro_data, sales_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sku_list = sales_data[sales_data.gto.isna()].sku.unique()\n",
    "target_brand_list = sales_data[sales_data.gto.isna()][[\"sku\", \"brand\"]].brand.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = sales_data[[\"sku\", \"brand\"]].drop_duplicates().groupby(\"sku\").apply(lambda x: set(x.brand.unique()))#[target_sku_list]\n",
    "k2 = sales_data[[\"sku\", \"brand\"]].drop_duplicates().groupby(\"sku\").apply(lambda x: x.brand.nunique()>1)\n",
    "\n",
    "issue_skus = k[k2].drop_duplicates().to_list()\n",
    "\n",
    "brand_date_range = sales_data.fillna(0).reset_index().groupby(\"brand\").apply(lambda x: pd.Series([x.date.min(), x.date.max()], index=[\"min\", \"max\"]))\n",
    "brand_date_range = brand_date_range.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(tuple(i)[0] in target_brand_list, tuple(i)[1] in target_brand_list) for i in issue_skus]\n",
    "# sales_data[[\"sku\", \"brand\"]].drop_duplicates().groupby(\"brand\").apply(lambda x: x.sku.nunique())[list(set.union(*issue_skus))]\n",
    "# [(brand_date_range[\"max\"][tuple(i)[0]], brand_date_range[\"max\"][tuple(i)[1]]) for i in issue_skus]\n",
    "# [(brand_date_range[\"min\"][tuple(i)[0]], brand_date_range[\"min\"][tuple(i)[1]]) for i in issue_skus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_data.reset_index().groupby([\"date\", \"brand\"]).volume.sum().unstack(1).T.loc[target_brand_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_data.reset_index().groupby([\"date\", \"sku\"]).volume.sum().unstack(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sales_data.reset_index().groupby([\"date\"])[[\"volume\", \"net_revenue\", \"promotional_discount\", \"other_discounts\"]].sum().sort_index().join(macro_data)\n",
    "k = df.net_revenue.shift(1)\n",
    "k.name = \"shifted_nr\"\n",
    "df = df.join(k).fillna(method=\"bfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.iloc[:-2].copy(deep=True)\n",
    "\n",
    "scaled_cols = [\"net_revenue\", \"promotional_discount\", \"other_discounts\", \"shifted_nr\"]#, \"private_consumption\", \"gross_domestic_saving\", \"brnd_money\", \"gdp\", \"shifted_nr\"]\n",
    "scaler = data.net_revenue.mean()\n",
    "data.loc[:,scaled_cols] = data.loc[:,scaled_cols]/scaler\n",
    "\n",
    "\n",
    "mixed_effect_cols = [\"retail_sales_index\", \"unemployment_rate\", \"cpi\", \"private_consumption\", \"gross_domestic_saving\", \"brad_money\", \"gdp\"]\n",
    "\n",
    "data.loc[:, mixed_effect_cols] = data.loc[:, mixed_effect_cols].divide(data.loc[:, mixed_effect_cols].mean())-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "# Y\n",
    "# y = tf.constant(data.net_revenue, dtype=tf.float64)\n",
    "# discounts = -tf.constant(data[[\"promotional_discount\", \"other_discounts\"]].values, dtype=tf.float64)\n",
    "# mixed_effect = tf.constant(data[mixed_effect_cols].values, dtype=tf.float64)\n",
    "# time_index = tf.constant(np.arange(1, data.shape[0]+1), dtype=tf.float64)\n",
    "# shifted_nr = tf.constant(data.shifted_nr, dtype=tf.float64)\n",
    "\n",
    "#Y\n",
    "y = tf.compat.v1.placeholder(dtype=tf.float64, name=\"y_actual\")\n",
    "\n",
    "# X\n",
    "shifted_nr = tf.compat.v1.placeholder(dtype=tf.float64, name=\"shifted_nr\")\n",
    "discounts = tf.compat.v1.placeholder(dtype=tf.float64, name=\"discounts\")\n",
    "mixed_effect = tf.compat.v1.placeholder(dtype=tf.float64, name=\"mixed_effects\")\n",
    "time_index = tf.compat.v1.placeholder(dtype=tf.float64, name=\"time_index\")\n",
    "\n",
    "\n",
    "\n",
    "# variables\n",
    "baseline_intercept = tf.Variable(1, dtype=tf.float64)\n",
    "baseline_slope1 = tf.Variable(1, dtype=tf.float64)\n",
    "baseline_slope2 = tf.Variable(1, dtype=tf.float64)\n",
    "mixed_effect_mult = tf.Variable(np.random.normal(loc=1, size=(1, 7)), dtype=tf.float64)\n",
    "discount_slope = tf.math.sigmoid(tf.Variable(np.random.normal(loc=-1, size=(1, 2)), dtype=tf.float64))*3\n",
    "roi_mults = tf.Variable(np.random.normal(loc=1, size=(1, 7)), dtype=tf.float64)\n",
    "\n",
    "variable_list = [baseline_intercept, baseline_slope1, baseline_slope2, mixed_effect_mult, discount_slope]\n",
    "\n",
    "# impacts\n",
    "base1 = tf.multiply(baseline_slope1, time_index) + baseline_intercept\n",
    "base2 = base1# + tf.multiply(baseline_slope2, shifted_nr)\n",
    "mixed_effect_impact = 1 + tf.nn.tanh(tf.multiply(mixed_effect, mixed_effect_mult))\n",
    "total_mixed_effect_impact = tf.reduce_prod(mixed_effect_impact, axis=1)\n",
    "discount_impact = tf.multiply(discount_slope, discounts)\n",
    "roi_mult_impact = 1 + tf.nn.tanh(tf.multiply(mixed_effect_impact, roi_mults))\n",
    "total_roi_mult_impact = tf.expand_dims(tf.reduce_prod(roi_mult_impact, axis=1), axis=1)\n",
    "\n",
    "# prediction\n",
    "y_pred = (\n",
    "    tf.multiply(base2, total_mixed_effect_impact)\n",
    "    + tf.reduce_sum(discount_impact, axis=1)\n",
    ")\n",
    "\n",
    "# loss\n",
    "wape = tf.reduce_sum(tf.math.abs(y - y_pred))/tf.reduce_sum(y)\n",
    "mse = tf.reduce_sum(tf.math.square(y - y_pred))\n",
    "reg = sum([tf.reduce_sum(tf.square(i)) for i in variable_list])\n",
    "\n",
    "loss = 1e3*wape + 1e2*mse + reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict1 = {\n",
    "    discounts : -data[[\"promotional_discount\", \"other_discounts\"]].iloc[:-5,:].astype(np.float64).values,\n",
    "    mixed_effect: data[mixed_effect_cols].iloc[:-5,:].astype(np.float64).values,\n",
    "    shifted_nr : data[\"shifted_nr\"].iloc[:-5].astype(np.float64).values,\n",
    "    y : data[\"net_revenue\"].iloc[:-5].astype(np.float64).values,\n",
    "    time_index : np.float64(np.arange(1, data.iloc[:-5,:].shape[0]+1)),\n",
    "}\n",
    "\n",
    "feed_dict2 = {\n",
    "    discounts : -data[[\"promotional_discount\", \"other_discounts\"]].iloc[-5:,:].astype(np.float64).values,\n",
    "    mixed_effect: data[mixed_effect_cols].iloc[-5:,:].astype(np.float64).values,\n",
    "    shifted_nr : data[\"shifted_nr\"].iloc[-5:].astype(np.float64).values,\n",
    "    y : data[\"net_revenue\"].iloc[-5:].astype(np.float64).values,\n",
    "    time_index : np.float64(np.arange(1, data.iloc[-5:,:].shape[0]+1)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "# optimizer\n",
    "lr = lambda x : 0.1 / np.power(x/100 + 10, 2 / 3)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=lr(epoch))#, beta1=0.1, beta2=0.1)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# initialize variables\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(init, feed_dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "num_epochs = 30000\n",
    "for epoch in range(num_epochs):\n",
    "    _, current_loss, current_wape, current_mse, current_reg = sess.run([train, loss, wape, mse, reg], feed_dict1)\n",
    "    if (epoch + 1) % 250 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {current_loss:.4f}, WAPE: {current_wape:.4f}, MSE: {current_mse:.4f}, reg: {current_reg:.4f}\")\n",
    "\n",
    "\n",
    "#         # Training loop\n",
    "# num_epochs = 500\n",
    "# for epoch in range(num_epochs):\n",
    "#     _, current_error, cuurent_mse, current_m1, current_m2, current_c = sess.run([train_op, error, mse_error, m1, m2, c])\n",
    "#     if (epoch + 1) % 25 == 0:\n",
    "#         print(f\"Epoch {epoch + 1}/{num_epochs}, Error: {current_error:.4f}, MSE: {cuurent_mse:.4f}, m1: {current_m1}, m2: {current_m2}, c: {current_c}\")\n",
    "\n",
    "# # Print the final results for 'm' and 'c'\n",
    "# final_m1, final_m2, final_c = sess.run([m1, m2, c])\n",
    "# print(f\"Final 'm1' value: {final_m1}\")\n",
    "\n",
    "# print(f\"Final 'm2' value: {final_m2}\")\n",
    "# print(f\"Final 'c' value: {final_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "num_epochs = 30000\n",
    "for epoch in range(num_epochs):\n",
    "    _, current_loss, current_wape, current_mse, current_reg = sess.run([train, loss, wape, mse, reg], feed_dict1)\n",
    "    if (epoch + 1) % 250 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {current_loss:.4f}, WAPE: {current_wape:.4f}, MSE: {current_mse:.4f}, reg: {current_reg:.4f}\")\n",
    "\n",
    "\n",
    "#         # Training loop\n",
    "# num_epochs = 500\n",
    "# for epoch in range(num_epochs):\n",
    "#     _, current_error, cuurent_mse, current_m1, current_m2, current_c = sess.run([train_op, error, mse_error, m1, m2, c])\n",
    "#     if (epoch + 1) % 25 == 0:\n",
    "#         print(f\"Epoch {epoch + 1}/{num_epochs}, Error: {current_error:.4f}, MSE: {cuurent_mse:.4f}, m1: {current_m1}, m2: {current_m2}, c: {current_c}\")\n",
    "\n",
    "# # Print the final results for 'm' and 'c'\n",
    "# final_m1, final_m2, final_c = sess.run([m1, m2, c])\n",
    "# print(f\"Final 'm1' value: {final_m1}\")\n",
    "\n",
    "# print(f\"Final 'm2' value: {final_m2}\")\n",
    "# print(f\"Final 'c' value: {final_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.absolute(sess.run(y, feed_dict1)-sess.run(y_pred, feed_dict1)).sum()/sess.run(y, feed_dict1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.absolute(sess.run(y, feed_dict2)-sess.run(y_pred, feed_dict2)).sum()/sess.run(y, feed_dict2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(sess.run(y, feed_dict1), sess.run(y_pred, feed_dict1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"y\":sess.run(y, feed_dict1), \"y_pred\" : sess.run(y_pred, feed_dict1)}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(variable_list, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(sess.run([base1, base2-base1, tf.multiply(base2, mixed_effect_impact)-base2, tf.reduce_sum(discount_impact, axis=1)])).T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the necessary libraries\n",
    "# import numpy as np\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Generate or load your dataset\n",
    "# X, y = df.iloc[:-2, 2:], df.iloc[:-2, 1]\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create a MinMaxScaler and fit it to the training data\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Create a linear regression model\n",
    "# model = LinearRegression()\n",
    "\n",
    "# # Fit the model to the scaled training data\n",
    "# model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Make predictions on the scaled test data\n",
    "# y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# # Calculate and print performance metrics\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "# print(\"R-squared:\", r2)\n",
    "\n",
    "# # Optionally, you can also access the model's coefficients and intercept\n",
    "# coefficients = model.coef_\n",
    "# intercept = model.intercept_\n",
    "\n",
    "# print(\"Coefficients:\", coefficients)\n",
    "# print(\"Intercept:\", intercept)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
